{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plotlyflask_server.data_parser import sort_RAFT_table as sRt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import re\n",
    "from scipy.optimize import curve_fit\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plotly forms in graphs do not work with jupyterlab 7 or higher out of the box (https://github.com/plotly/plotly.py/issues/4336) so the following lines are sometimes necessary:\n",
    "import plotly.io as pio\n",
    "from functools import wraps\n",
    "# another fix is installing jupyterlab_mathjax2 or viewing it with html in browser\"\n",
    "\n",
    "def iframe_decorator(func):\n",
    "    @wraps(func)\n",
    "    def inner(*args, **kwargs):\n",
    "        pio.renderers.default = \"iframe\"\n",
    "        out = func(*args, **kwargs)\n",
    "        pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "        return out\n",
    "    return inner"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# rectify/balance the table to the same amount of conversion and mass columns for easier data manipulation\n",
    "sRt.df[\"t6h-conversion\"] = np.nan\n",
    "sRt.df[\"t10h-conversion\"] = np.nan\n",
    "\n",
    "# get all conversion headers, sort them by the hours\n",
    "conversion_list = []\n",
    "for column in sRt.df.columns:\n",
    "    if \"conversion\" in column:\n",
    "        conversion_list.append(column)\n",
    "\n",
    "conversion_list.sort(key=lambda x: int(re.findall(r\"\\d+\", x)[0]))\n",
    "\n",
    "# get all the Mn and Mw headers\n",
    "Mn_list = []\n",
    "Mw_list = []\n",
    "pdi_list = []\n",
    "for column in sRt.df.columns:\n",
    "    if \"Mn\" in column:\n",
    "        Mn_list.append(column)\n",
    "    if \"Mw\" in column:\n",
    "        Mw_list.append(column)\n",
    "    if \"Ð\" in column:\n",
    "        pdi_list.append(column)\n",
    "\n",
    "print(conversion_list, Mn_list, Mw_list, pdi_list, sep=\"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# change time format from hours to minutes for better overview of sampling times in the later kinetic plots\n",
    "def change_time_format(time_format):\n",
    "    h_m_s = str(time_format).split(\":\")\n",
    "    m_format = int(h_m_s[0]) * 60 + int(h_m_s[1]) + int(h_m_s[2]) / 60\n",
    "    return m_format\n",
    "\n",
    "def change_time_format_h(time_format):\n",
    "    h_m_s = str(time_format).split(\":\")\n",
    "    h_format = int(h_m_s[0]) + (int(h_m_s[1]) + int(h_m_s[2]) / 60) /60\n",
    "    return h_format\n",
    "\n",
    "# creating a table as a lookup to correct all sample measurement times\n",
    "\n",
    "# get the hours from the headers with regex\n",
    "hours_list = []\n",
    "two_digit_regex = r\"\\d+\"\n",
    "for column in conversion_list:\n",
    "    hours_list.append(int(re.findall(two_digit_regex, column)[0]))\n",
    "hours_list.sort()\n",
    "\n",
    "exact_times = pd.read_excel(sRt.INPUT_FILE_PATH, sheet_name=\"exact sampling times\")\n",
    "\n",
    "time_correction_df = pd.DataFrame(data=exact_times.iloc[3:12, 3:])\n",
    "time_correction_df.columns = exact_times.loc[2][3:]\n",
    "\n",
    "time_correction_df.reset_index(inplace=True, drop=True)\n",
    "ext_time_corr_df_data =[]\n",
    "for row in time_correction_df.iterrows():\n",
    "    reactor_nr = row[1][\"Reactor\"]\n",
    "    if type(reactor_nr) == int or reactor_nr == \"closing of reactors\": # that applies to reactor 15 and the closing of reactors\n",
    "        row[1][\"Reactor\"] = str(reactor_nr)\n",
    "        ext_time_corr_df_data.append(row[1])\n",
    "    else: # that applies to all reactors which are described per row in pairs like \"3+4\"\n",
    "        reactor_nr_s = (row[1][\"Reactor\"]).split(\"+\")\n",
    "        for reactor_nr in reactor_nr_s:\n",
    "            row[1][\"Reactor\"] = reactor_nr\n",
    "            ext_time_corr_df_data.append(row[1].copy())\n",
    "ext_time_corr_df = pd.DataFrame(data=ext_time_corr_df_data, columns=exact_times.loc[2][3:])\n",
    "ext_time_corr_df.reset_index(drop=True, inplace=True)\n",
    "print(\"The timer was reset to 0 after the first closing of reactors in the \\\"t = 0\\\"-sampling-step\")\n",
    "ext_time_corr_df.columns = [\"Reactor\", 0, 1, 2, 4, 6, 8, 10, 15]\n",
    "\n",
    "# change time format to hours\n",
    "time_cols = ext_time_corr_df.columns.difference([\"Reactor\"])\n",
    "ext_time_corr_df[time_cols] = ext_time_corr_df[time_cols].apply(lambda x: [change_time_format_h(d) for d in x])\n",
    "ext_time_corr_df[0] = ext_time_corr_df[0].apply(lambda x: 0 )\n",
    "\n",
    "# set index to reactor\n",
    "ext_time_corr_df.set_index(\"Reactor\", inplace=True)\n",
    "\n",
    "ext_time_corr_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# replace the \"sample determiner\" with columns describing for the experiment number (next cell) and the actual reagents that made out the \"determiner\"\n",
    "reaction_descriptors_dict = {}\n",
    "abbreviation_keys = pd.read_excel(sRt.INPUT_FILE_PATH, sheet_name=\"Legend for Abbreviations\")\n",
    "abbreviation_keys.dropna(how=\"all\", inplace=True)\n",
    "for row in abbreviation_keys.itertuples():\n",
    "    reaction_descriptors_dict[str(row.Symbol)] = row.Name\n",
    "\n",
    "reaction_descriptors_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sRt.df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# generate a reformatted table with all entries attributing to a sample analysis taken, described with a column of the right time\n",
    "kinetic_curves = []\n",
    "for index, polymerisation_kinetic in sRt.df.iterrows():\n",
    "\n",
    "    kinetic_curve_entries = pd.DataFrame(index=range(len(polymerisation_kinetic[conversion_list])),\n",
    "        data={\"time\" : hours_list, \"conversion\" : polymerisation_kinetic[conversion_list].values,\n",
    "              \"Mn\" : polymerisation_kinetic[Mn_list].values, \"Mw\" : polymerisation_kinetic[Mw_list].values,\n",
    "              \"dispersity\" : polymerisation_kinetic[pdi_list].values\n",
    "              })\n",
    "\n",
    "    kinetic_curve_entries[\"exp_nr\"] = str(polymerisation_kinetic[\"Experiment number\"])\n",
    "    kinetic_curve_entries[\"monomer\"] = reaction_descriptors_dict[polymerisation_kinetic[\"monomer\"]]\n",
    "    kinetic_curve_entries[\"RAFT-Agent\"] = reaction_descriptors_dict[polymerisation_kinetic[\"RAFT-Agent\"]]\n",
    "    kinetic_curve_entries[\"solvent\"] = reaction_descriptors_dict[polymerisation_kinetic[\"solvent\"]]\n",
    "\n",
    "    # the times are dependent on the current reactor, get current\n",
    "    current_reactor_nr = str(polymerisation_kinetic[\"reactor\"])\n",
    "    current_time_list = ext_time_corr_df.loc[current_reactor_nr]\n",
    "    kinetic_curve_entries[\"time\"] = list(current_time_list)\n",
    "\n",
    "    kinetic_curves.append(kinetic_curve_entries)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "'''\n",
    "deprecated: more theoretical accurate version but practically worse applicable\n",
    "    # L is responsible for scaling the output range from [0,1] to [0,L]\n",
    "    # b and x0 are responsible for shifting the the function on the x and y axis\n",
    "    # k is responsible for scaling the input, which remains in (-inf,inf)\n",
    "'''\n",
    "\n",
    "def sigmoid (x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0)))+b\n",
    "    return (y)\n",
    "\n",
    "def sigmoid_derivative(x, L ,x0, k, b):\n",
    "    y = (L*k*np.exp(-k*(x-x0)))/((np.exp(-k*(x-x0))+1)**2)\n",
    "    return (y)\n",
    "\n",
    "def neg_growth(x, l, k):\n",
    "    y = l * (1 - np.exp(k * (-x)))\n",
    "    return y\n",
    "\n",
    "def neg_growth_derivative(x, l, k):\n",
    "    y = l * k * np.exp(k*(-x))\n",
    "    return y\n",
    "\n",
    "# as the Mn values do not all start at 0\n",
    "def neg_growth_abscissa(x, l, k, b):\n",
    "    y = l * (1 - np.exp(k * (-x))) + b\n",
    "    return y\n",
    "\n",
    "def linear_growth(x, m):\n",
    "    y = m * x\n",
    "    return y\n",
    "\n",
    "def linear_growth_derivative(m):\n",
    "    return m\n",
    "\n",
    "'''\n",
    "xrange = np.arange(-0.2, 5, 0.1)\n",
    "example_fig = px.line()\n",
    "for testparams in ([1,1], [2,1], [1,2]):\n",
    "    example_fig.add_scatter(x=xrange, y=neg_growth(xrange, *testparams))\n",
    "    example_fig.add_scatter(x=xrange, y=neg_growth_derivative(xrange, *testparams), opacity=0.5, line=dict(dash=\"dot\"))\n",
    "\n",
    "print(\"visualisation of the functions\")\n",
    "example_fig.show()\n",
    "# '''\n",
    "print(\"functions loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# checking out max and min of Mn/Mw and Ð\n",
    "from plotly.subplots import make_subplots\n",
    "min_Mn, max_Mn = [], []\n",
    "min_Mw, max_Mw = [], []\n",
    "min_pdi, max_pdi = [], []\n",
    "\n",
    "for kc in kinetic_curves:\n",
    "    min_Mn.append(kc[\"Mn\"].min()); max_Mn.append(kc[\"Mn\"].max())\n",
    "    min_Mw.append(kc[\"Mw\"].min()); max_Mw.append(kc[\"Mw\"].max())\n",
    "    min_pdi.append(kc[\"dispersity\"].min()); max_pdi.append(kc[\"dispersity\"].max())\n",
    "\n",
    "for _ in (min_Mn, max_Mn, min_Mw, max_Mw):\n",
    "    _.sort()\n",
    "    _ = [j/100000 for j in _]\n",
    "\n",
    "for _ in (min_pdi, max_pdi):\n",
    "    _.sort()\n",
    "\n",
    "molar_mass_distribution_fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "molar_wg_comparison = px.line()\n",
    "molar_wg_comparison.add_scatter(y=min_Mw, name=\"min Mw\", line_color=\"darkgoldenrod\", opacity=0.8, fill=\"tozeroy\", fillcolor=\"orange\")\n",
    "molar_wg_comparison.add_scatter(y=max_Mw, name=\"max Mw\", line_color=\"darkgoldenrod\", opacity=0.8)\n",
    "molar_wg_comparison.add_scatter(y=min_Mn, name=\"min Mn\", line_color=\"firebrick\", opacity=0.8, fill=\"tozeroy\", fillcolor=\"red\")\n",
    "molar_wg_comparison.add_scatter(y=max_Mn, name=\"max Mn\", line_color=\"firebrick\", opacity=0.8)\n",
    "# molar_wg_comparison.update_layout(title=\"Mn and Mw comparison\", yaxis_title=r\"$\\\\text{Mn and Mw }[g/mol] \\cdot 10^{-5}$\")\n",
    "\n",
    "pdi_comparison = px.line()\n",
    "pdi_comparison.add_scatter(y=min_pdi, name=\"min Ð\", line_color=\"cornflowerblue\", opacity=0.8, fill=\"tozeroy\", fillcolor=\"rgba(0, 0, 255, 0.2)\")\n",
    "pdi_comparison.add_scatter(y=max_pdi, name=\"max Ð\", line_color=\"cornflowerblue\", opacity=0.8)\n",
    "pdi_comparison.update_traces(yaxis=\"y2\")\n",
    "\n",
    "molar_mass_distribution_fig.add_traces(molar_wg_comparison.data + pdi_comparison.data)\n",
    "molar_mass_distribution_fig.update_layout(title=\"Mn, Mw and Ð comparison\", yaxis_title=r\"$\\\\text{Mn and Mw }[g/mol] \\cdot 10^{-5}$\", yaxis2_title=\"Ð\")\n",
    "\n",
    "# pio.renderers.default = \"jupyterlab\"\n",
    "molar_mass_distribution_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "kin_curve_fits_fig = px.line(title=\"Kinetic Curve Fit\")\n",
    "colors = px.colors.qualitative.Plotly # set up a simple color palette\n",
    "extended_xdata = np.linspace(-1, 16.5, 100) # x data array for plotting the fits\n",
    "\n",
    "\n",
    "def add_fits_to_plot(figure, fit_func, fit_func_params,  fit_func_derivative=None, ext_xdata=extended_xdata, *args, **kwargs):\n",
    "    figure.add_scatter(\n",
    "        x=ext_xdata, y=fit_func(ext_xdata, *fit_func_params),\n",
    "        opacity=1, line=dict(dash=\"dot\"), name=f\"{fit_func.__name__} fit\", *args, **kwargs)\n",
    "    if fit_func_derivative:\n",
    "        figure.add_scatter(\n",
    "            x=ext_xdata, y=fit_func_derivative(ext_xdata, *fit_func_params),\n",
    "            opacity=0.3, line=dict(dash=\"dash\"), name=f\"{fit_func_derivative.__name__}\", *args, **kwargs)\n",
    "\n",
    "def fit_and_exclude_outliers(x, y, fit_func, p0, bounds, nan_policy=\"omit\", iteration=1, outliers=None):\n",
    "    outliers = outliers if outliers is not None else []\n",
    "\n",
    "    # exclude the nan values from the data\n",
    "    mask = ~np.isnan(y)\n",
    "    x, y = x[mask], y[mask]\n",
    "\n",
    "    cf_data = curve_fit(f=fit_func, xdata=x, ydata=y, p0=p0, nan_policy=nan_policy, maxfev=800 * 10, bounds=bounds)\n",
    "\n",
    "    # calculate the fit points\n",
    "    fit_points = np.array([fit_func(x, *cf_data[0]) for x in x])\n",
    "    # calculate the standard deviation of the residuals between the fit and the data points\n",
    "    sigma = np.std(fit_points - y)\n",
    "\n",
    "    # exclude the outliers\n",
    "    msk = ~(np.abs(fit_points - y) > 2 * sigma)\n",
    "    if not msk.all():\n",
    "        return fit_and_exclude_outliers(x=x[msk], y=y[msk], fit_func=fit_func, p0=cf_data[0], bounds=bounds, nan_policy=nan_policy, iteration=iteration+1, outliers=outliers + [x[~msk]])\n",
    "\n",
    "    # calculate the squared error of fit and data points\n",
    "    sq_err = np.mean(np.square(y - fit_points))\n",
    "\n",
    "    result = {\"x\": x, \"y\": y, \"p_opt\" : cf_data[0], \"p_cov\" : cf_data[1], \"sq_err\": sq_err, \"excluded_points\" : (x[~msk],y[~msk]), \"iteration\" : iteration, \"outliers\" : outliers}\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# rephrased question from reviewer 3: \"how good do all three different fitting functions fit the data in comparison?\"\n",
    "# to answer this question we will fit the data with all three functions and compare the r² values of the fits. Then we'll see if we can categorize some of the monomers/RAFT-agents/solvents to a specific fit type.\n",
    "r_squared_errors = [[], [], []] # three lists for the three fitting functions\n",
    "fitting_function = [linear_growth, sigmoid, neg_growth] # the three fitting functions\n",
    "initial_guesses = [[1], [max, 1, 1, 0], [max, 0.1]] # initial guesses for the fitting functions\n",
    "bounds = [([-np.inf], [np.inf]), ([0, -np.inf, -np.inf, -np.inf], [1, np.inf, np.inf, np.inf]), ([0, -np.inf], [1, np.inf])] # bounds for the fitting functions\n",
    "for idx, kinetic_curve in enumerate(kinetic_curves):\n",
    "    # first make sure the datapoints are in the right format and not sometimes int sometimes float\n",
    "    xdata = np.array(kinetic_curve[\"time\"].values, dtype=float)\n",
    "    ydata = np.array(kinetic_curve[\"conversion\"].values, dtype=float)\n",
    "\n",
    "    for fit_idx, fit_func in enumerate(fitting_function):\n",
    "        p_initial = [_ if not callable(_) else _(ydata) for _ in initial_guesses[fit_idx]] # if the initial guess has a function, call it\n",
    "        ng_fit = fit_and_exclude_outliers(x=xdata, y=ydata, fit_func=fit_func, p0=p_initial, bounds=bounds[fit_idx])\n",
    "        r_squared_errors[fit_idx].append(ng_fit[\"sq_err\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# calculate the amount of with which of the fit-types is the best\n",
    "exp_nums = [_[\"exp_nr\"].iloc[0] for _ in kinetic_curves]\n",
    "best_fit = [0, 0, 0]\n",
    "for exp, lin, sig, n_growth in zip(exp_nums, *r_squared_errors):\n",
    "    if lin < sig and lin < n_growth:\n",
    "        best_fit[0] += 1\n",
    "    elif sig < lin and sig < n_growth:\n",
    "        best_fit[1] += 1\n",
    "    else:\n",
    "        best_fit[2] += 1\n",
    "print(f\"The best fit for the experiments is:\\n\"\n",
    "      f\"Linear growth: {best_fit[0]} times with {np.mean(r_squared_errors[0]):.4f} mean and {sum(r_squared_errors[0]):.4f} in sum\\n\"\n",
    "      f\"Sigmoid growth: {best_fit[1]} times with {np.mean(r_squared_errors[1]):.4f} mean and {sum(r_squared_errors[1]):.4f} in sum\\n\"\n",
    "      f\"Negative growth: {best_fit[2]} times with {np.mean(r_squared_errors[2]):.4f} mean and {sum(r_squared_errors[2]):.4f} in sum\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the r² values of the fit-types\n",
    "r_squared_fig = px.line(title=\"R² values of the curve fittings\", labels={\"x\":\"Kinetic Curve\", \"y\":\"R² value\"})\n",
    "r_squared_fig.add_scatter(y=r_squared_errors[0], x=exp_nums, name=\"Linear growth fit\", line_color=\"blue\")\n",
    "r_squared_fig.add_scatter(y=r_squared_errors[1], x=exp_nums, name=\"Sigmoid fit\", line_color=\"green\")\n",
    "r_squared_fig.add_scatter(y=r_squared_errors[2], x=exp_nums, name=\"Negative growth fit\", line_color=\"red\")\n",
    "r_squared_fig.update_layout(\n",
    "    paper_bgcolor='white',  # Set background to light mode\n",
    "    plot_bgcolor='white',   # Set plot background to white\n",
    "    font=dict(color='black', size=15),  # Set font color to black for light mode\n",
    "    margin=dict(l=0, r=0, t=90, b=10),  # Tight margins to reduce whitespace\n",
    "    width=1000, #height=300,\n",
    "    # turn off the grid\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showline=True, linewidth=2, linecolor='black'),\n",
    "    # move the legend to the top left corner\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.0,\n",
    "        xanchor=\"right\",\n",
    "        x=1.0,\n",
    "        bgcolor='rgba(255, 255, 255, 0.8)',  # Semi-transparent white background for the legend\n",
    "        bordercolor='black',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    # reinsert axis description\n",
    "    xaxis_title=\"Experiment number\",\n",
    "    yaxis_title=\"R² value\",\n",
    ")\n",
    "r_squared_fig.write_image(os.path.join(os.getcwd(), \"data\", \"data exploration figures\", \"r_squared of the fits.svg\"))\n",
    "r_squared_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# categorize the monomers/RAFT-agents/solvents by the best fit type\n",
    "import copy\n",
    "reaction_descriptors = list(reaction_descriptors_dict.values()) # get the reaction descriptors in a list of strings\n",
    "types_in_fit ={\"monomer\":{_:0 for _ in reaction_descriptors[1:17]},\n",
    "               \"RAFT-Agent\":{_:0 for _ in reaction_descriptors[17:27]},\n",
    "               \"solvent\":{_:0 for _ in reaction_descriptors[27:30]}}\n",
    "types_in_lin_fit = copy.deepcopy(types_in_fit)\n",
    "types_in_sig_fit = copy.deepcopy(types_in_fit)\n",
    "types_in_neg_fit = copy.deepcopy(types_in_fit)\n",
    "\n",
    "for idx, kinetic_curve in enumerate(kinetic_curves):\n",
    "    exp_nr = kinetic_curve[\"exp_nr\"].iloc[0]\n",
    "    if r_squared_errors[0][idx] < r_squared_errors[1][idx] and r_squared_errors[0][idx] < r_squared_errors[2][idx]:\n",
    "        types_in_lin_fit[\"monomer\"][kinetic_curve[\"monomer\"].iloc[0]] += 1\n",
    "        types_in_lin_fit[\"RAFT-Agent\"][kinetic_curve[\"RAFT-Agent\"].iloc[0]] += 1\n",
    "        types_in_lin_fit[\"solvent\"][kinetic_curve[\"solvent\"].iloc[0]] += 1\n",
    "    elif r_squared_errors[1][idx] < r_squared_errors[0][idx] and r_squared_errors[1][idx] < r_squared_errors[2][idx]:\n",
    "        types_in_sig_fit[\"monomer\"][kinetic_curve[\"monomer\"].iloc[0]] += 1\n",
    "        types_in_sig_fit[\"RAFT-Agent\"][kinetic_curve[\"RAFT-Agent\"].iloc[0]] += 1\n",
    "        types_in_sig_fit[\"solvent\"][kinetic_curve[\"solvent\"].iloc[0]] += 1\n",
    "    else:\n",
    "        types_in_neg_fit[\"monomer\"][kinetic_curve[\"monomer\"].iloc[0]] += 1\n",
    "        types_in_neg_fit[\"RAFT-Agent\"][kinetic_curve[\"RAFT-Agent\"].iloc[0]] += 1\n",
    "        types_in_neg_fit[\"solvent\"][kinetic_curve[\"solvent\"].iloc[0]] += 1\n",
    "\n",
    "# remove the monomers/RAFT-agents/solvents that were not used in a fit at all\n",
    "types_in_lin_fit = {k: {kk: vv for kk, vv in v.items() if vv > 0} for k, v in types_in_lin_fit.items()}\n",
    "types_in_sig_fit = {k: {kk: vv for kk, vv in v.items() if vv > 0} for k, v in types_in_sig_fit.items()}\n",
    "types_in_neg_fit = {k: {kk: vv for kk, vv in v.items() if vv > 0} for k, v in types_in_neg_fit.items()}\n",
    "print(types_in_lin_fit, types_in_sig_fit, types_in_neg_fit, sep=\"\\n\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "kinetics_df = pd.DataFrame(columns=['exp_nr', 'max_con', 'theo_max_con', 'theo_react_end', 'max_mn', 'monomer',\n",
    "       'RAFT-Agent', 'solvent', 'fit_p1', 'fit_p2', 'p1_variance',\n",
    "       'p1_p2_covariance', 'p2_variance', 'squared_error', 'conv_time_data',\n",
    "       'Mn_time_data', 'Mw_time_data', 'dispersity_time_data']) # create new dataframe with kinetics per row\n",
    "\n",
    "for idx, kinetic_curve in enumerate(kinetic_curves):\n",
    "    # first make sure the datapoints are in the right format and not sometimes int sometimes float\n",
    "    xdata = np.array(kinetic_curve[\"time\"].values, dtype=float)\n",
    "    ydata_conv = np.array(kinetic_curve[\"conversion\"].values, dtype=float)\n",
    "    ydata_dispersity = np.array(kinetic_curve[\"dispersity\"].values, dtype=float)\n",
    "    ydata_Mn = np.array(kinetic_curve[\"Mn\"].values, dtype=float)/100000 # make it more comparable to conversion\n",
    "    ydata_Mw = np.array(kinetic_curve[\"Mw\"].values, dtype=float)/100000\n",
    "\n",
    "    # fitting section for conversion\n",
    "    p_initial = [max(ydata_conv), 0.1] # this is a mandatory initial guess\n",
    "    ng_fit = fit_and_exclude_outliers(x=xdata, y=ydata_conv, fit_func=neg_growth, p0=p_initial, bounds=([0, -np.inf], [1, np.inf]))\n",
    "    # l_fit = fit_and_exclude_outliers(x=xdata, y=ydata, fit_func=linear_growth, p0=[max(ydata)/7], bounds=([0], [np.inf]))\n",
    "\n",
    "    popt, pcov = ng_fit[\"p_opt\"], ng_fit[\"p_cov\"]\n",
    "    conv_time_data = np.array([ng_fit[\"x\"], ng_fit[\"y\"]])\n",
    "    squared_error = ng_fit[\"sq_err\"]\n",
    "\n",
    "    # fitting section for Mn\n",
    "    max_Mn = max(ydata_Mn[~np.isnan(ydata_Mn)]) \n",
    "    p_initial = [max_Mn, 0.1, 0]\n",
    "    ng_fit_Mn = fit_and_exclude_outliers(x=xdata, y=ydata_Mn, fit_func=neg_growth_abscissa, p0=p_initial, bounds=([0, -np.inf, -np.inf], [1, np.inf, np.inf]))\n",
    "\n",
    "    popt_Mn, pcov_Mn = ng_fit_Mn[\"p_opt\"], ng_fit_Mn[\"p_cov\"]\n",
    "    Mn_time_data = np.array([ng_fit_Mn[\"x\"], ng_fit_Mn[\"y\"]])\n",
    "    squared_error_Mn = ng_fit_Mn[\"sq_err\"]\n",
    "\n",
    "    # fitting section for Mw\n",
    "    ng_fit_Mw = fit_and_exclude_outliers(x=xdata, y=ydata_Mw, fit_func=neg_growth_abscissa, p0=p_initial, bounds=([0, -np.inf, -np.inf], [1, np.inf, np.inf]))\n",
    "\n",
    "    popt_Mw, pcov_Mw = ng_fit_Mw[\"p_opt\"], ng_fit_Mw[\"p_cov\"]\n",
    "    Mw_time_data = np.array([ng_fit_Mw[\"x\"],ng_fit_Mw[\"y\"]])\n",
    "    squared_error_Mw = ng_fit_Mw[\"sq_err\"]\n",
    "    \n",
    "    # combining dispersity with times omitting Nan\n",
    "    nan_val_in_d_mask = ~np.isnan(ydata_dispersity)\n",
    "    dispersity_time_data = np.array([xdata[nan_val_in_d_mask], ydata_dispersity[nan_val_in_d_mask]])\n",
    "\n",
    "\n",
    "    kinetics_df.loc[idx] = {\"exp_nr\":kinetic_curve[\"exp_nr\"].iloc[1], \"max_con\":max(ydata_conv),\n",
    "                            \"theo_max_con\":\"yet to calc\", \"theo_react_end\":\"yet to calc\", \"max_mn\":max_Mn,\n",
    "                            \"monomer\":kinetic_curve[\"monomer\"].iloc[1], \"RAFT-Agent\":kinetic_curve[\"RAFT-Agent\"].iloc[1],\n",
    "                            \"solvent\":kinetic_curve[\"solvent\"].iloc[1],\n",
    "                            \"fit_p1\":popt[0],\"fit_p2\":popt[1],\n",
    "                            \"p1_variance\":pcov[0][0],\"p1_p2_covariance\":pcov[0][1],\"p2_variance\":pcov[1][1],\n",
    "                            \"squared_error\":squared_error, \"conv_time_data\":conv_time_data,\n",
    "                            \"Mn_time_data\":Mn_time_data, \"Mw_time_data\":Mw_time_data,\n",
    "                            \"dispersity_time_data\": dispersity_time_data}\n",
    "\n",
    "''' comment out here if plot is needed.\n",
    "    marker_dict = dict(color=colors[idx%len(colors)]) # set same colors per kinetic\n",
    "    kin_curve_fits_fig.add_scatter(x=xdata, y=ydata, mode=\"lines+markers\", opacity=1, name=kinetic_curve[\"exp_nr\"].iloc[1], marker=marker_dict)\n",
    "    add_fits_to_plot(neg_growth, neg_growth_derivative, popt, marker_dict)\n",
    "\n",
    "kin_curve_fits_fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        range=[-0.1,1]\n",
    "    ),\n",
    "\n",
    ")\n",
    "kin_curve_fits_fig.show()\n",
    "# '''\n",
    "\n",
    "kinetics_df.reset_index(drop=True, inplace=True)\n",
    "len_kinetics = len(kinetics_df)\n",
    "kinetics_df.drop(axis=\"index\", index=kinetics_df[kinetics_df[\"max_con\"] <= 0].index, inplace=True)\n",
    "kinetics_dropped_bec_max_con = len_kinetics-len(kinetics_df)\n",
    "print(f\"{kinetics_dropped_bec_max_con} kinetics data points dropped because max con was less than 0\")\n",
    "kinetics_df.reset_index(drop=True, inplace=True)\n",
    "# kinetics_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "data_type_list = [\"conv_time_data\", \"Mn_time_data\", \"Mw_time_data\", \"dispersity_time_data\"]\n",
    "data_list_name = [\"conv over time\", \"Mn over time\", \"Mw over time\", \"dispersity over time\"]\n",
    "def plot_Mn_Conversion_Dispersity_difference():\n",
    "    mn_conversion_diff_fig = px.line(title=\"Mn Conversion difference\")\n",
    "    for idx, data_type in (enumerate(data_type_list)):\n",
    "        for kin in kinetics_df[data_type].head(40):\n",
    "            mn_conversion_diff_fig.add_scatter(x=kin[0], y=kin[1], mode=\"lines+markers\", opacity=0.5, marker=dict(color=colors[idx%len(colors)]), showlegend=False, legendgroup=str(idx))\n",
    "        # custom legend\n",
    "        mn_conversion_diff_fig.add_scatter(x=[None], y=[None], marker=dict(color=colors[idx%len(colors)]), name= data_list_name[idx], legendgroup=str(idx))\n",
    "    # adding a threshold line because the SEC peaks smear into syspeak at that point\n",
    "    mn_conversion_diff_fig.add_scatter(x=[0, 16], y=[1000*10**(-5), 1000*10**(-5)], mode=\"lines\", line=dict(dash=\"dash\"), name=\"SEC limit\", legendgroup=\"threshold\")\n",
    "    mn_conversion_diff_fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.05, xanchor=\"left\", x=-0.1 ), yaxis_title=r\"$\\text{Mn, Mw }[g/mol] \\cdot 10^{-5}/ \\ [ \\%]$\", xaxis_title=\"Time [h]\")\n",
    "    print(\"saving resources to plot for conversion Mn and Mw of the first 40 kinetics\")\n",
    "    mn_conversion_diff_fig.show()\n",
    "plot_Mn_Conversion_Dispersity_difference()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# normalize the errors by dividing them by their respective standard deviation\n",
    "def normalize_errors(err):\n",
    "    return err / np.std(err)\n",
    "\n",
    "for error in [\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"]:\n",
    "    kinetics_df[error] = normalize_errors(kinetics_df[error])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# kinetics_df[kinetics_df[\"squared_error\"]>0.01] 29/317 entries\n",
    "# calculating covariance between errors to use only the reasonable ones\n",
    "\n",
    "# permutate all combinations of the errors\n",
    "errorcombs = [[],[],[]]\n",
    "for err1, err2 in itertools.combinations([\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"], 2):\n",
    "    # print(f\"The covariance between {err1} and {err2} is {np.cov(kinetics_df[err1], kinetics_df[err2])}\")\n",
    "    # print(f\"The correlation between {err1} and {err2} is {np.correlate(kinetics_df[err1], kinetics_df[err2])}\")\n",
    "    # print(\"\\n\")\n",
    "    errorcombs[0].append(f\"{err1}/{err2}\")\n",
    "    errorcombs[1].append(np.cov(kinetics_df[err1], kinetics_df[err2]))\n",
    "    errorcombs[2].append(*np.correlate(kinetics_df[err1], kinetics_df[err2]))\n",
    "errorcombs_dc = {\"name\": errorcombs[0], \"covariance\":errorcombs[1], \"correlation\":errorcombs[2]}\n",
    "errorcombs_df = pd.DataFrame(data=errorcombs_dc)\n",
    "errorcombs_df[\"i_correlation\"] = errorcombs_df[\"correlation\"] * (-1)\n",
    "\n",
    "# plot bar plot per a pair of errors with superimposed correlation\n",
    "err_fig = px.bar(title=\"Correlation between Errors\", labels={\"correlation\":\"correlation\"}, log_y=True)\n",
    "err_fig.add_bar(x=errorcombs_df[\"name\"], y=errorcombs_df[\"correlation\"], name=\"positive correlation\", marker_color=\"green\")\n",
    "err_fig.add_bar(x=errorcombs_df[\"name\"], y=errorcombs_df[\"i_correlation\"], name=\"negative correlation\", marker_color=\"crimson\")\n",
    "err_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# violin plots of the errors\n",
    "for errors in [\"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"]:\n",
    "    kinetics_df[errors] = kinetics_df[errors].apply(lambda x: np.absolute(x))\n",
    "violin_err_fig = px.violin(kinetics_df, y=[\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"], box=True, points=\"all\", log_y=True)\n",
    "violin_err_fig.update_layout(title=\"Errors normalize by \\u03C3\", xaxis_title=\"Error type\", yaxis_title=\"value\", yaxis=dict(title=\"log(value)\", range=(-22, 3)))\n",
    "violin_err_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# get information about within which error margin the fits and therefore kinetics are good\n",
    "# split error in quartiles, index per error\n",
    "\n",
    "# make sure the data exploration figures folder exists\n",
    "if not os.path.exists(\"data/data exploration figures\"):\n",
    "    os.makedirs(\"data/data exploration figures\")\n",
    "\n",
    "title_dic = {\"squared_error\":\"r² - error\",\n",
    "             \"p1_variance\":\"variance of parameter 1\",\n",
    "             \"p2_variance\":\"variance of parameter 2\",\n",
    "             \"p1_p2_covariance\":\"covariance (Params 1 & 2)\"}\n",
    "\n",
    "def get_quartile_indexes(error_type):\n",
    "    quartile_len = len(kinetics_df) / 4\n",
    "    quartile_ranges = np.array([(a * quartile_len, (a + 1) * quartile_len) for a in range(4)], dtype=int)\n",
    "    quartiles_list = list()\n",
    "    for q in range(4):\n",
    "        quartiles_list.append(kinetics_df.sort_values(by=[error_type]).iloc[quartile_ranges[q][0]:quartile_ranges[q][1]])\n",
    "    return quartiles_list\n",
    "\n",
    "fig = px.line()\n",
    "def export_quartile_figures():\n",
    "    for err in tqdm([\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"]):\n",
    "        err_quartiles = get_quartile_indexes(err)\n",
    "\n",
    "        # plot the single quartiles\n",
    "        for nr, n in enumerate(err_quartiles):\n",
    "            iks = n.index\n",
    "            fg = px.line(title=f\"kinetics (quartile {nr+1} of the {title_dic[err]}) \", labels={\"x\":\"time\", \"y\":\"conversion\"})\n",
    "            for ik in iks:\n",
    "                marker_dict = dict(color=colors[ik%len(colors)])\n",
    "                fit_data = kinetics_df.iloc[ik]\n",
    "                xdt, ydt = fit_data[\"xdata\"], fit_data[\"ydata\"]\n",
    "                fg.add_scatter(x=xdt, y=ydt, mode=\"lines+markers\", name=kinetic_curves[ik][\"exp_nr\"].iloc[0], marker=marker_dict)\n",
    "                add_fits_to_plot(fig, neg_growth, neg_growth_derivative, [fit_data[\"fit_p1\"], fit_data[\"fit_p2\"]], marker=marker_dict)\n",
    "                fg.update_layout(yaxis=dict(range=[-0.1,1]), xaxis_title=\"Time [h]\", yaxis_title=\"Conversion [%]\")\n",
    "            fg.write_image(f\"data/data exploration figures/{err} ({nr+1} quartile).svg\")\n",
    "\n",
    "# export_quartile_figures() ; print(\"exported\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# descry when a fitting function aligns to the datapoints in a reasonable way.\n",
    "# a point for every quartile further from the first (with the greatest error) for each single error is given.\n",
    "# lastly that score is normalized by dividing by the maximum score (that is 3*4=12)\n",
    "error_list = [\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"]\n",
    "error_dic = {}\n",
    "score = np.zeros(len(kinetics_df), int)\n",
    "for error in error_list:\n",
    "    quartiles = get_quartile_indexes(error)\n",
    "    # for every error we want to give an error per index\n",
    "    for sc, quartile in enumerate(quartiles):\n",
    "        if sc == 0:\n",
    "            continue\n",
    "        # each quartile is a dataframe, where the latter one raise a higher error ( 0, 1, 2, 3)\n",
    "        for num in quartile.index:\n",
    "            score[num]+=sc\n",
    "kinetics_df[\"error_score\"]=score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "p1_values = kinetics_df[\"fit_p1\"].values\n",
    "average_conversion = p1_values.mean()\n",
    "min_con, max_con = p1_values.min(), p1_values.max()\n",
    "\n",
    "print(f\"According to the fits the mean maximum conversion is {average_conversion:.3n}, the minimum maximum is {min_con:.3n} and the maximum overall is {max_con:.3n}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# determining apposite reaction end values.\n",
    "# The moment where 90% of the maximum conversion have been reached can be seen as a practical maximum conversion point\n",
    "kinetics_df[\"theo_max_con\"] = kinetics_df[\"fit_p1\"].apply(lambda x: x*0.9)\n",
    "\n",
    "def y_converted_negative_growth(y, l, k):\n",
    "    return -np.log(1 - y / l) / k\n",
    "\n",
    "kinetics_df[\"theo_react_end\"] = \\\n",
    "    [y_converted_negative_growth(y, fit_p1, fit_p2) for y, fit_p1, fit_p2 in zip(kinetics_df[\"theo_max_con\"], kinetics_df[\"fit_p1\"], kinetics_df[\"fit_p2\"])]\n",
    "\n",
    "# the theoretical maximal conversion must be capped at reasonable time (we take two days here) that is applying 139/313 entries\n",
    "kinetics_df[\"theo_react_end\"] = [30 if x > 30 else x for x in kinetics_df[\"theo_react_end\"].values]\n",
    "# recalculate the apposite maximal conversion\n",
    "kinetics_df[\"theo_max_con\"] = [neg_growth(x, p1, p2) for x, p1, p2 in zip(kinetics_df[\"theo_react_end\"], kinetics_df[\"fit_p1\"], kinetics_df[\"fit_p2\"])]\n",
    "# find the mean dispersity of the last 3 values (there are always at least 4), as the first low molar weights are usually too uncertain due to SEC sensitivity\n",
    "kinetics_df[\"mean_dispersity\"] = kinetics_df[\"dispersity_time_data\"].apply(lambda x: np.mean(x[1][-3:]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# searching manually for interesting experiments to be reproduced by MR.\n",
    "search_queue_monomer = kinetics_df[\"monomer\"].apply(lambda x: x in [\"Styrene\",  \"4-Methylstyrene\", \"Benzyl acrylate\"])\n",
    "search_queue_solvent = kinetics_df[\"solvent\"].apply(lambda x: x in [\"Dimethylformamide\"])\n",
    "kinetics_df[search_queue_monomer & search_queue_solvent]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# one longrun test for the theo react end cap (146) for up to 72h\n",
    "# two to see reproducible exp from within the second quantile in average (score of 4/5)\n",
    "#   one \"fast\" fit point (202)\n",
    "#   one for a weak conversion (90)\n",
    "kinetics_df[kinetics_df['exp_nr'].isin([\"205\", \"90\", \"146\"])]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# to find the optimal threshold parameters for search one has to keep in mind that with high conversion (assuming\n",
    "# around 80%) increasing side reactions can take place. After conversion the reactions should be sorted after time,\n",
    "# then error score and lastly polymer dispersity. A multiple-decreasing-threshold-sorting-algorithm would be good:\n",
    "# So first priority would be sorting after nearest to 80% conversion.\n",
    "\n",
    "# create a score ingesting the importance of the different kinetic descriptors\n",
    "#     Conversion*1 + time²*(-0.8) + error_score*(0.5) + dispersity*(0.3)\n",
    "#     while spanning between the optimum and the least bearable values like in the following:\n",
    "#          Conversion: |con-0.8| - 0 (0.8 is the optimum)\n",
    "#             using a linear decreasing function -x*m+b\n",
    "#          Time: 0 - np.inf (0 is the optimum) (more than 30 is not bearable and set as maximum)\n",
    "#             using a negative potential function -x**2+b\n",
    "#          Error: 0 - 12 (0 is the optimum) (the error is more negligible)\n",
    "#             using a linear decreasing function -x*m+b\n",
    "#          Dispersity: 1 is the optimum, at 1.5 half the score should be lost. The further from there the less impact.\n",
    "#             using a reciprocal function 1/(2x-1)\n",
    "score = []\n",
    "for row in kinetics_df.itertuples():\n",
    "    row_score = ((0.8-np.abs(row.theo_max_con - 0.8))/0.8*1 + (-(row.theo_react_end/30)**2+1)*0.8 + ((12-row.error_score)/12)*0.5) + (1/(2*row.mean_dispersity-1)*0.3)\n",
    "    normalized_row_score = row_score / ( 1 + 0.8 + 0.5 + 0.3)\n",
    "    score.append(normalized_row_score)\n",
    "kinetics_df[\"score\"] = score\n",
    "kinetics_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def color_variant(hex_color, brightness_offset=1):\n",
    "    \"\"\" takes a color like #87c95f and produces a lighter or darker variant \"\"\"\n",
    "    if len(hex_color) != 7:\n",
    "        raise Exception(\"Passed %s into color_variant(), needs to be in #87c95f format.\" % hex_color)\n",
    "    rgb_hex = [hex_color[x:x+2] for x in [1, 3, 5]]\n",
    "    new_rgb_int = [int(hex_value, 16) + brightness_offset for hex_value in rgb_hex]\n",
    "    new_rgb_int = [min([255, max([0, i])]) for i in new_rgb_int] # make sure new values are between 0 and 255\n",
    "    # hex() produces \"0x88\", we want just \"88\"; also we zfill to pad with leading zeros if necessary, e.g. 9 -> 09\n",
    "    return \"#\" + \"\".join([hex(i)[2:].zfill(2) for i in new_rgb_int])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Some search functions:\n",
    "def search_for_exp(exp_nr: str | list) -> pd.DataFrame:\n",
    "    if type(exp_nr) == str:\n",
    "        print(f\"Searching for experiment {exp_nr}\")\n",
    "\n",
    "        return kinetics_df[kinetics_df[\"exp_nr\"] == exp_nr]\n",
    "    else:\n",
    "        return kinetics_df[kinetics_df[\"exp_nr\"].isin(exp_nr)]\n",
    "\n",
    "def plot_exp(exp_nr: str | list, plot_mn: bool=False, plot_mw: bool=False, fit_curves: tuple=(True, True)):\n",
    "    plot_data = search_for_exp(exp_nr)\n",
    "    exp_fig = px.line(title=f\"Kinetic Curve Fit for {exp_nr}\")\n",
    "    for kinetic_to_plot in plot_data.itertuples():\n",
    "        x_data, ydata = kinetic_to_plot.conv_time_data\n",
    "        marker_dict = dict(color=colors[int(kinetic_to_plot.Index)%len(colors)])\n",
    "        exp_fig.add_scatter(x=x_data, y=ydata, mode=\"lines+markers\", name=kinetic_to_plot.exp_nr, marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr))\n",
    "        if fit_curves[0]:\n",
    "            if fit_curves[1]:\n",
    "                add_fits_to_plot(exp_fig, neg_growth, [kinetic_to_plot.fit_p1, kinetic_to_plot.fit_p2], fit_func_derivative=neg_growth_derivative, marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr), showlegend=False)\n",
    "            else:\n",
    "                add_fits_to_plot(exp_fig, neg_growth, [kinetic_to_plot.fit_p1, kinetic_to_plot.fit_p2], marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr), showlegend=False)\n",
    "\n",
    "        if plot_mn:\n",
    "            marker_dict[\"color\"] = color_variant(marker_dict[\"color\"], 30)\n",
    "            x2_data, y2_data = kinetic_to_plot.Mn_time_data\n",
    "            exp_fig.add_scatter(x=x2_data, y=y2_data, mode=\"lines+markers\", name=\"Mn of \" + kinetic_to_plot.exp_nr, marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr))\n",
    "        if plot_mw:\n",
    "            marker_dict[\"color\"] = color_variant(marker_dict[\"color\"], -60)\n",
    "            x2_data, y2_data = kinetic_to_plot.Mw_time_data\n",
    "            exp_fig.add_scatter(x=x2_data, y=y2_data, mode=\"lines+markers\", name=\"Mw of \" + kinetic_to_plot.exp_nr, marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr), opacity=0.5)\n",
    "\n",
    "    exp_fig.update_layout(yaxis=dict(range=[-0.1,1]), xaxis_title=\"Time [h]\", yaxis_title=\"Conversion [%]\")\n",
    "    exp_fig.show()\n",
    "\n",
    "def find_optimal_synthesis(monomer: str | list):\n",
    "    search_q_monomer = [x in monomer for x in kinetics_df[\"monomer\"]]\n",
    "    result_df = kinetics_df[search_q_monomer].sort_values(by=[\"score\"], ascending=False)\n",
    "    return result_df\n",
    "\n",
    "def refine_search(dataframe: pd.DataFrame, monomer: list = None, solvent: list = None, raft_agent: list = None):\n",
    "    len_df = len(dataframe)\n",
    "    search_q_monomer, search_q_solvent, search_q_raft_agent = [np.array([True] * len_df) for _ in range(3)]\n",
    "    if monomer:\n",
    "        search_q_monomer = dataframe[\"monomer\"].apply(lambda x: x in [*monomer])\n",
    "    if solvent:\n",
    "        search_q_solvent = dataframe[\"solvent\"].apply(lambda x: x in [*solvent])\n",
    "    if raft_agent:\n",
    "        search_q_raft_agent = dataframe[\"RAFT-Agent\"].apply(lambda x: x in [*raft_agent])\n",
    "    return dataframe[search_q_monomer & search_q_solvent & search_q_raft_agent]\n",
    "\n",
    "# search_for_exp([\"145\", \"90\", \"253\"])\n",
    "# plot_exp([\"145\", \"90\", \"253\"], True, True)\n",
    "plot_exp([\"241\", \"146\", \"392\"], True, True)\n",
    "# find_optimal_synthesis([\"Styrene\", \"4-Methylstyrene\", \"Benzyl acrylate\"])\n",
    "# refine_search(kinetics_df, solvent=[\"Dimethylformamide\"], monomer=[\"Styrene\", \"4-Methylstyrene\", \"Benzyl acrylate\"], raft_agent=[\"2-Cyan-2-propylbenzodithioat\"])\n",
    "# refine_search(find_optimal_synthesis([\"Styrene\", \"4-Methylstyrene\", \"Benzyl acrylate\"]), solvent=[\"Dimethylformamide\"], raft_agent=[\"2-Cyan-2-propylbenzodithioat\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from IPython.display import Markdown, display\n",
    "def display_color(hex_color, width=6):\n",
    "    if type(hex_color) == str:\n",
    "        display(Markdown(f'<span style=\"font-family: monospace\">{hex_color} <span style=\"color: {hex_color}\">{chr(9608)*width}</span></span>'))\n",
    "    else:\n",
    "        display(Markdown('<br>'.join(\n",
    "            f'<span style=\"font-family: monospace\">{color} <span style=\"color: {color}\">{chr(9608)*width}</span></span>'\n",
    "            for color in hex_color)))\n",
    "display_color(colors)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# manually typing in the proof experimental data from michael\n",
    "# \"MRG-046-G-ZL-1-E-DMF\" -> exp\n",
    "# \"MRG-046-G-ZL-4-B-DMF\" -> exp\n",
    "# \"MRG-046-G-ZL-15-F-DMF\" -> exp\n",
    "# \"exp\", \"monomer\", \"RAFT-Agent\", \"solvent\", \"time\", \"conversion\", \"Mn\", \"Mw\"\n",
    "\n",
    "\n",
    "\"\"\" !The experiment numbers changed to from 146, 90 and 205 to 241, 145 and 343!\n",
    "M. Ringleb:\n",
    "146 evaluation of the observations was done after ca. 90 hno addition of NMR solvent and SEC eluent to vials for 15 h sample -->\n",
    "potential explanation for problems with evaluation or deviations (sample stood for 5 h without being quenched) --> for NMR addition of 400 yL of CDCl3 prior to filling to NMR tube\n",
    "\n",
    "90 evaluation of this points was done after 20 h time in the reactor\n",
    "double addition of NMR solvent and SEC eluent to the vials for t =10h\n",
    "also for NMR sampling for t=10 h sample the septum was pushed through the lid, so the vial stood open for ca. 9hours before it was filled to the NMR tube \"\n",
    "\n",
    "205 evaluation of this points was done after 20 h time in the reactor\"\"\"\n",
    "\n",
    "proof_data = [\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 0, 0.0, 0.0, 0.0],\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 15, 0.05, 3000, 3400],\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 60, 0.24, 3500, 3900],\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 72, 0.24, 3500, 4100],\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 84, 0.24, 3700, 4200],\n",
    "    [\"145b\", \"4\", \"B\", \"DMF\", 0, 0.0, 0.0, 0.0],\n",
    "    [\"145b\", \"4\", \"B\", \"DMF\", 2.95, 0.02, 870, 970],\n",
    "    [\"145b\", \"4\", \"B\", \"DMF\", 10, 0.10, 2200, 2500],\n",
    "    [\"343b\", \"15\", \"F\", \"DMF\", 0, 0.0, 0.0, 0.0],\n",
    "    [\"343b\", \"15\", \"F\", \"DMF\", 5.62, 0.85, 7700, 14800],\n",
    "    [\"343b\", \"15\", \"F\", \"DMF\", 8, 0.90, 7100, 15100],\n",
    "    ]\n",
    "\n",
    "# calculating dispersity\n",
    "for i in range(len(proof_data)):\n",
    "    if proof_data[i][7] == 0:\n",
    "        proof_data[i].append(0)\n",
    "    else:\n",
    "        proof_data[i].append(proof_data[i][6]/proof_data[i][7])\n",
    "\n",
    "proof_df_points = pd.DataFrame(data=proof_data, columns=[\"exp_nr\", \"monomer\", \"RAFT-Agent\", \"solvent\", \"time\", \"conversion\", \"Mn\", \"Mw\", \"dispersity\"])\n",
    "# reformatting to kinetics_df format\n",
    "proof_df = pd.DataFrame()\n",
    "for kinetic_nr in proof_df_points[\"exp_nr\"].unique():\n",
    "    kinetic = proof_df_points[proof_df_points[\"exp_nr\"] == kinetic_nr]\n",
    "    proof_kinetic = pd.DataFrame({\"exp_nr\":kinetic[\"exp_nr\"].iloc[0],\n",
    "                                  \"monomer\":reaction_descriptors_dict[kinetic[\"monomer\"].iloc[0]],\n",
    "                                  \"RAFT-Agent\":reaction_descriptors_dict[kinetic[\"RAFT-Agent\"].iloc[0]],\n",
    "                                  \"solvent\":reaction_descriptors_dict[kinetic[\"solvent\"].iloc[0]],\n",
    "\n",
    "                                  \"conv_time_data\":[[kinetic[\"time\"].values, kinetic[\"conversion\"].values]],\n",
    "                                  \"Mn_time_data\":[[kinetic[\"time\"].values, kinetic[\"Mn\"].values/100000]],\n",
    "                                  \"Mw_time_data\":[[kinetic[\"time\"].values, kinetic[\"Mw\"].values/100000]],\n",
    "                                  \"dispersity_time_data\":[[kinetic[\"time\"].values, kinetic[\"dispersity\"].values]],})\n",
    "\n",
    "    proof_df = pd.concat([proof_df, proof_kinetic])\n",
    "proof_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# comparing proof and original experiments data\n",
    "proof_fig = px.line()\n",
    "\n",
    "for idx, data in (enumerate(data_type_list)):\n",
    "    comb_d = pd.concat([proof_df, search_for_exp([\"241\", \"145\", \"343\"])])\n",
    "    for kin, exp_nr in zip(comb_d[data], comb_d[\"exp_nr\"]):\n",
    "        proof_fig.add_scatter(x=kin[0], y=kin[1], mode=\"lines+markers\", opacity=0.5, name=exp_nr, marker=dict(color=colors[idx%len(colors)]), showlegend=False, legendgroup=str(idx))\n",
    "    # custom legend\n",
    "    proof_fig.add_scatter(x=[None], y=[None], marker=dict(color=colors[idx%len(colors)]), name=data_list_name[idx], legendgroup=str(idx))\n",
    "# adding a threshold line because the SEC peaks smear into syspeak at that point\n",
    "proof_fig.add_scatter(x=[0, 84], y=[1000*10**(-5), 1000*10**(-5)], mode=\"lines\", line=dict(dash=\"dash\"), name=\"SEC limit\", legendgroup=\"threshold\")\n",
    "proof_fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.05, xanchor=\"left\", x=-0.1 ), yaxis_title=r\"$Mn, Mw \\ [g/mol] \\cdot 10^{-5}\\text{ and conversion} \\ [ \\%]$\", xaxis_title=\"Time [h]\",\n",
    "                        paper_bgcolor='white',  # Set background to light mode\n",
    "                        plot_bgcolor='white',   # Set plot background to white\n",
    "                        font=dict(color='black'),  # Set font color to black for light mode\n",
    "                        margin=dict(l=0, r=0, t=10, b=0),  # Tight margins to reduce whitespace\n",
    "                        width=500,\n",
    "                        ) #height=300,\n",
    "                        \n",
    "proof_fig.show()\n",
    "\n",
    "\"\"\"\n",
    "Y. Köster:\n",
    "The Comparison of proof and prior data shows that the experimental data is not easily reproducible with respective accuracy in small orders of magnitude (Conversion after 15 h around 10 %)\n",
    "Especially for the RAFT synthesis with this concern of low reaction rate an extrapolation further than double the time (to 30 h) seems not feasible.\n",
    " Therefore the former time cap of 70 h gets changed to 30 h.\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# I want to compare \"long-term experiment\" and \"low and high conversion reproducibility\" separately\n",
    "# 241b and 241 are necessary for the prior\n",
    "# 145, 343 and their b's are for the later\n",
    "comparisons = [[\"241\"], [\"145\", \"343\"]]\n",
    "# datatypes to compare from data_type_list = [\"conv_time_data\", \"Mn_time_data\", \"Mw_time_data\"]\n",
    "# @iframe_decorator\n",
    "def compare_redone_exp_mw(repl_exp_nr: list):\n",
    "    comp_fig = px.line()\n",
    "    comb_df = pd.concat([search_for_exp(repl_exp_nr), proof_df[[nr in [c + \"b\" for c in repl_exp_nr] for nr in proof_df[\"exp_nr\"]]]])\n",
    "    \n",
    "    max_time = 0 # to get the lenght of the sec limit line\n",
    "    for idx, data in (enumerate([data_type_list[2]])):\n",
    "        for kin, exp_nr in zip(comb_df[data], comb_df[\"exp_nr\"]):\n",
    "            comp_fig.add_scatter(x=kin[0], y=kin[1]*10**5, mode=\"markers+lines\", opacity=1, name=exp_nr,)# marker=dict(color=colors[idx%len(colors)]), showlegend=False, legendgroup=str(idx))\n",
    "            max_time = max(max_time, kin[0][-1])\n",
    "            \n",
    "            # adding fitting only to 241\n",
    "            if exp_nr == \"241\":\n",
    "                xdata = np.array(kin[0], dtype=float)\n",
    "                ydata_Mw = np.array(kin[1]*10**5, dtype=float)\n",
    "                p_initial = [max(ydata_Mw), 0.1]\n",
    "                ng_fit_Mw = fit_and_exclude_outliers(x=xdata, y=ydata_Mw, fit_func=neg_growth, p0=p_initial, bounds=([0, -np.inf], [np.inf, np.inf]))\n",
    "                comp_fig.add_scatter(x=np.linspace(-1, 80, 100), y=neg_growth(np.linspace(-1, 80, 100), *[ng_fit_Mw[\"p_opt\"][0], ng_fit_Mw[\"p_opt\"][1]]),\n",
    "                                     opacity=1, line=dict(dash=\"dot\"), name=\"Prediction\")\n",
    "                \n",
    "        # custom legend\n",
    "        # comp_fig.add_scatter(x=[None], y=[None], marker=dict(color=colors[idx%len(colors)]), name=data_list_name[idx], legendgroup=str(idx))\n",
    "    # adding a threshold line because the SEC peaks smear into syspeak at that point\n",
    "    comp_fig.add_scatter(x=[0, max_time], y=[1000, 1000], mode=\"lines\", line=dict(dash=\"dash\"), name=\"SEC limit\", legendgroup=\"threshold\")\n",
    "    comp_fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.05, xanchor=\"left\", x=-0.1 ), yaxis_title=\"<i>M</i><sub>w</sub> [g mol<sup>-1</sup>]\", xaxis_title=\"Time [h]\",\n",
    "                           paper_bgcolor='white',  # Set background to light mode\n",
    "                           plot_bgcolor='white',   # Set plot background to white\n",
    "                           font=dict(color='black'),  # Set font color to black for light mode\n",
    "                           margin=dict(l=0, r=0, t=10, b=0),  # Tight margins to reduce whitespace\n",
    "                           width=500, height=250,\n",
    "                           xaxis=dict(showgrid=False),\n",
    "                           yaxis=dict(showgrid=True, gridcolor=\"rgba(0,0,0,0.2)\", gridwidth=1),\n",
    "                           )\n",
    "    \n",
    "    return comp_fig\n",
    "\n",
    "# compare_redone_exp_mw(comparisons[0]).write_image(os.path.join(os.getcwd(), \"data\", \"data exploration figures\", \"long-therm-experiment-mw.svg\"))\n",
    "compare_redone_exp_mw(comparisons[0]).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compare_redone_exp_conv(repl_exp_nr: list):\n",
    "    comp_fig = px.line()\n",
    "    # comb_df = pd.concat([search_for_exp(repl_exp_nr), proof_df[[nr in [c + \"b\" for c in repl_exp_nr] for nr in proof_df[\"exp_nr\"]]]])  just to have the legend side by side: 145 145b 343 343b\n",
    "    comb_df = pd.concat([search_for_exp(\"145\"), proof_df[proof_df[\"exp_nr\"]==\"145b\"], search_for_exp(\"343\"), proof_df[proof_df[\"exp_nr\"]==\"343b\"]])\n",
    "\n",
    "    max_time = 0 # to get the lenght of the sec limit line\n",
    "    color_wheel = 0\n",
    "    for idx, data in (enumerate([data_type_list[0]])):\n",
    "        for kin, exp_nr in zip(comb_df[data], comb_df[\"exp_nr\"]):\n",
    "            comp_fig.add_scatter(x=kin[0], y=kin[1]*100, mode=\"markers\", opacity=1, name=exp_nr, marker=dict(color=colors[color_wheel%len(colors)]))# marker=dict(color=colors[idx%len(colors)]), showlegend=False, legendgroup=str(idx))\n",
    "            color_wheel += 1\n",
    "            max_time = max(max_time, kin[0][-1])\n",
    "            # add fitting to the first of the experimentation pars\n",
    "            if exp_nr == \"145\" or exp_nr == \"343\":\n",
    "                comp_xdata = np.array(kin[0], dtype=float)\n",
    "                comp_ydata_conv = np.array(kin[1]*100, dtype=float)\n",
    "                comp_p_initial = [max(ydata_conv), 0.1]\n",
    "                comp_ng_fit_conv = fit_and_exclude_outliers(x=comp_xdata, y=comp_ydata_conv, fit_func=neg_growth,\n",
    "                                                            p0=comp_p_initial, bounds=([0, -np.inf], [np.inf, np.inf]))\n",
    "                # add_fits_to_plot(comp_fig, neg_growth, [comp_ng_fit_conv[\"p_opt\"][0], comp_ng_fit_conv[\"p_opt\"][1]])\n",
    "                # print(comp_ng_fit_conv[\"p_opt\"], comp_ng_fit_conv[\"outliers\"])\n",
    "                comp_fig.add_scatter(\n",
    "                    x=np.linspace(0, int(kin[0][-1]+1), 100),\n",
    "                    y=neg_growth(\n",
    "                        np.linspace(0, int(kin[0][-1]+1), 100), *[comp_ng_fit_conv[\"p_opt\"][0],  comp_ng_fit_conv[\"p_opt\"][1]]),\n",
    "                    opacity=0.5, line=dict(dash=\"dot\"), showlegend=False, marker=dict(color=\"gray\"))\n",
    "        # custom legend\n",
    "        # comp_fig.add_scatter(x=[None], y=[None], marker=dict(color=colors[idx%len(colors)]), name=data_list_name[idx], legendgroup=str(idx))\n",
    "    # adding a threshold line because the SEC peaks smear into syspeak at that point\n",
    "    # comp_fig.add_scatter(x=[0, max_time], y=[1000*10**(-5), 1000*10**(-5)], mode=\"lines\", line=dict(dash=\"dash\"), name=\"SEC limit\", legendgroup=\"threshold\") # r\"$Mn\\ [g/mol] \\cdot 10^{-5}$\"\n",
    "    comp_fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.05, xanchor=\"left\", x=-0.1 ), yaxis_title=\"Conversion [%]\", xaxis_title=\"Time [h]\",\n",
    "                            paper_bgcolor='white',  # Set background to light mode\n",
    "                            plot_bgcolor='white',   # Set plot background to white\n",
    "                            font=dict(color='black'),  # Set font color to black for light mode\n",
    "                            margin=dict(l=0, r=0, t=10, b=0),  # Tight margins to reduce whitespace\n",
    "                            width=500, height=250,\n",
    "                           xaxis=dict(showgrid=False),\n",
    "                           yaxis=dict(showgrid=True, gridcolor=\"rgba(0,0,0,0.2)\", gridwidth=1),\n",
    "                           )    \n",
    "    return comp_fig\n",
    "# compare_redone_exp_conv(comparisons[1]).write_image(os.path.join(os.getcwd(), \"data\", \"data exploration figures\", \"reproducibility_at_low_conversion.svg\"))\n",
    "compare_redone_exp_conv(comparisons[1]).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": "# search for all kinetics with falling Mn, Mw Values, they start appearing a lot when the Mn is over 0.25 * 10**5. They got now all curated out in the initial curation step.",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# how many number unique did it actually make through curation\n",
    "interests = [\"monomer\", \"RAFT-Agent\", \"solvent\"]\n",
    "for interest in interests:\n",
    "    print(str(kinetics_df[interest].nunique()) + \" for \" + interest)\n",
    "    print(kinetics_df[interest].unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "refine_search(dataframe=kinetics_df, raft_agent=[\"Benzyl 1H-pyrrole-1-carbodithioate\"])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# plot_exp([\"241\", \"145\", \"343\"], True, True)\n",
    "search_for_exp([\"241\", \"145\", \"343\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# create three 2D permutation tables (for each solvent)\n",
    "# I want to create the three tables with same headers (on first row and first column) and then fill them by iterating through the table and the discarded table to show which where tried, which \"worked\", which did not and where thus discarded and which are not tried. \n",
    "\n",
    "# Since there are fewer agents than monomers prior will be the filling the first row\n",
    "same_row_headers = [str(x) for x in range(1,17)] # just the index\n",
    "same_column_headers = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"] \n",
    "DMF_p_table = pd.DataFrame(data=np.zeros((16, 10), dtype=int), columns=same_column_headers, index=same_row_headers)\n",
    "Tol_p_table = pd.DataFrame(data=np.zeros((16, 10), dtype=int), columns=same_column_headers, index=same_row_headers)\n",
    "DMSO_p_table = pd.DataFrame(data=np.zeros((16, 10), dtype=int), columns=same_column_headers, index=same_row_headers)\n",
    "\n",
    "# fill the tables\n",
    "def fill_dataset_into_tables(dataset: pd.DataFrame, fill: int):\n",
    "    for row in dataset.itertuples():\n",
    "        monomer = row.monomer\n",
    "        agent = row._4 # since the - of RAFT-Agent is not allowed as an attribute name\n",
    "        solvent = row.solvent\n",
    "        match solvent:\n",
    "            case \"DMF\":\n",
    "                DMF_p_table.loc[monomer, agent] += fill\n",
    "            case \"Tol\":\n",
    "                Tol_p_table.loc[monomer, agent] += fill\n",
    "            case \"DMSO\":\n",
    "                DMSO_p_table.loc[monomer, agent] += fill\n",
    "\n",
    "fill_dataset_into_tables(sRt.df, 10)\n",
    "fill_dataset_into_tables(sRt.discarded_df, -1)\n",
    "# fill_dataset_into_tables(sRt.permutations_df, -3)\n",
    "# looking at e.g \"4\" \"A\" as monomer['4-Methylstyrene' raft_agent '2-Cyan-2-propylbenzodithioat' 4 experiments were introduced but 3 were discarded\n",
    "# a reaction that worked once will now just be set as \"working whilst \n",
    "def value_format(val):\n",
    "    if val > 0:\n",
    "        return True\n",
    "    elif val == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "DMSO_p_table = DMSO_p_table.map(value_format)\n",
    "Tol_p_table = Tol_p_table.map(value_format)\n",
    "DMF_p_table = DMF_p_table.map(value_format)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# write permutation tables to file\n",
    "# with pd.ExcelWriter(os.path.join(os.getcwd(), \"data\", \"data exploration figures\", \"permutation tables.xlsx\")) as writer:\n",
    "#     DMSO_p_table.to_excel(writer, sheet_name=\"DMSO\", index=True)\n",
    "#     Tol_p_table.to_excel(writer, sheet_name=\"Toluene\", index=True)\n",
    "#     DMF_p_table.to_excel(writer, sheet_name=\"DMF\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# re-involve the abortive experiments with a score of 0\n",
    "# 1st get the experiments from the discarded_df\n",
    "# 2nd count those (number-)unique and bring them in the same shape as the kinetics_df:\n",
    "#   translate monomer and agent numbers and letters\n",
    "#   leave conversion, time, score and so forth 0\n",
    "# 3rd append them to the kinetics_df\n",
    "# 4th drop all duplicate from the discarded only so no failed experiments will be shown twice when re-inserted: \n",
    "#   combine both, drop all duplicates for monomer, solvent, RAFT-Agent\n",
    "#   keep the remaining exp_nr and mask with them what should be left in the reformatted_discarded and drop the rest\n",
    "#   finally concat the filtered_reformatted_discarded and the kinetics_df\n",
    "reformatted_discarded = sRt.discarded_df[[\"Experiment number\", \"monomer\", \"RAFT-Agent\", \"solvent\"]].copy()\n",
    "# rename and reorder columns\n",
    "reformatted_discarded.columns = [\"exp_nr\", \"monomer\", \"RAFT-Agent\", \"solvent\"]\n",
    "reformatted_discarded[\"exp_nr\"] = reformatted_discarded[\"exp_nr\"].astype(str)\n",
    "mappables = [\"monomer\", \"RAFT-Agent\", \"solvent\"]\n",
    "reformatted_discarded[mappables] = reformatted_discarded[mappables].map(lambda x: reaction_descriptors_dict[x])\n",
    "\n",
    "reformatted_discarded[\"score\"] = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "discarded_kinetics = len(reformatted_discarded)\n",
    "discarded_kinetics_w_o_duplicates = len(reformatted_discarded.drop_duplicates(subset=[\"monomer\", \"RAFT-Agent\", \"solvent\"], keep=\"first\"))\n",
    "print(f\"{discarded_kinetics} discarded kinetics\")\n",
    "print(f\"{discarded_kinetics - discarded_kinetics_w_o_duplicates} (additional) duplicates in those discarded kinetics\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# append the reformatted discarded table above to the kinetics data\n",
    "# this series withholds all the exp numbers that should still be present in the discarded set\n",
    "exp_nr_to_keep = pd.concat([kinetics_df, reformatted_discarded.drop_duplicates(subset=[\"monomer\", \"RAFT-Agent\", \"solvent\"], keep=\"first\")]).drop_duplicates(subset=[\"monomer\", \"RAFT-Agent\", \"solvent\"], keep=False)[\"exp_nr\"]\n",
    "# create a mask\n",
    "reform_to_keep = reformatted_discarded[\"exp_nr\"].isin(exp_nr_to_keep)\n",
    "unique_discarded_non_over_all_duplicate = len(reformatted_discarded[reform_to_keep])\n",
    "print(f\"{unique_discarded_non_over_all_duplicate} discarded experiments which where unique over all\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_df = pd.concat([kinetics_df, reformatted_discarded[reform_to_keep]])\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "success_f_kinetics = len(kinetics_df)\n",
    "success_f_kinetics_w_o_duplicates = len(kinetics_df.drop_duplicates(subset=[\"monomer\", \"RAFT-Agent\", \"solvent\"], keep=\"first\"))\n",
    "print(f\"{success_f_kinetics} successful and reasonable kinetics\")\n",
    "print(f'{success_f_kinetics-success_f_kinetics_w_o_duplicates} (additional) duplicates')\n",
    "print(f\"{success_f_kinetics_w_o_duplicates} unique successful and reasonable kinetics\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# make a pie chart for the different dataparts\n",
    "# difference to evaluation table xlsx kinetics_dropped_bec_max_con = len_kinetics-len(kinetics_df) = 3 \n",
    "# we got 552 (311+241) kinetics in total,\n",
    "# 241 accurately described successful reaction kinetics  (~313 in skript before~)\n",
    "# 314 discarded (~311 in skript before, of which 198 (311-113) are duplicates~) \n",
    "              # \"dk unique over sk & dk\": discarded_kinetics_w_o_duplicates\n",
    "# sun_entries = ['Successful', 'Unique ', 'Duplicates ', 'Discarded', 'Unique', 'Duplicates']\n",
    "sun_entries = ['Successful (44%)', 'Unique (35%)', 'Duplicates (9%)', 'Discarded (56%)', 'Unique (33%)', 'Duplicates (23%)']\n",
    "sun_values = [success_f_kinetics,\n",
    "              success_f_kinetics_w_o_duplicates,\n",
    "              success_f_kinetics - success_f_kinetics_w_o_duplicates,\n",
    "              discarded_kinetics,\n",
    "              discarded_kinetics_w_o_duplicates,\n",
    "              discarded_kinetics - discarded_kinetics_w_o_duplicates,\n",
    "              ]\n",
    "# data_point_parents=[\"\", \"Successful\", \"Successful\", \"\", \"Discarded\", \"Discarded\"]\n",
    "data_point_parents=[\"\", \"Successful (44%)\", \"Successful (44%)\", \"\", \"Discarded (56%)\", \"Discarded (56%)\"]\n",
    "\n",
    "# sun_entries.append(\"unique over all discarded\"); sun_values.append(unique_discarded_non_over_all_duplicate); data_point_parents.append([\"Successful\", \"Discarded\"])\n",
    "\n",
    "kin_in_database_fig = go.Figure(go.Sunburst(\n",
    "    labels=sun_entries,\n",
    "    parents=data_point_parents,\n",
    "    values=sun_values,\n",
    "    branchvalues=\"total\",\n",
    "    insidetextorientation=\"horizontal\",\n",
    "    rotation=180\n",
    "))\n",
    "kin_in_database_fig.update_layout(title={\n",
    "        'text': \"Kinetic experiments: <br> successful, discarded, redone or unique\",\n",
    "        'y': 0.9,\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'\n",
    "    },\n",
    "    paper_bgcolor='white',  # Set background to light mode\n",
    "    plot_bgcolor='white',   # Set plot background to white\n",
    "    font=dict(color='black', size=15),  # Set font color to black for light mode\n",
    "    margin=dict(l=0, r=0, t=90, b=10),  # Tight margins to reduce whitespace\n",
    "    width=500, #height=300,\n",
    ")\n",
    "\n",
    "# Customize tick marks (if relevant on radial axes)\n",
    "kin_in_database_fig.update_traces(\n",
    "    marker=dict(line=dict(color='black', width=0.5)),  # Add outlines for the slices\n",
    "    textinfo=\"label+value\",  # Display labels and values\n",
    "    hoverinfo=\"label+value+percent entry\",  # Show relevant hover information\n",
    ")\n",
    "# kin_in_database_fig.write_image(os.path.join(os.getcwd(), \"data\", \"data exploration figures\", \"kin_in_database_fig.svg\"))\n",
    "kin_in_database_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# which \"solvent\", \"RAFT-Agent\", \"monomer\" work good in general:\n",
    "# px.sunburst(kinetics_df, path= [\"solvent\", \"RAFT-Agent\", \"monomer\"], values=\"max_con\", maxdepth=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import functools\n",
    "# theoretical, semi-practical and practical reaction fit\n",
    "def sigmoid (x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0)))+b\n",
    "    return (y)\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "# prefill functions\n",
    "fitting_functions = [(\"Theoretical - linear\", functools.partial(linear_growth, m=1)),\n",
    "                     (\"Semi-practical - sigmoidal\", functools.partial(sigmoid, L=0.9, x0=0.5, k=10, b=0)),\n",
    "                     (\"Practical - negative growth\", functools.partial(neg_growth, l=1, k=2))]\n",
    "\n",
    "fitting_function_comparison_fig = px.line(labels={\"x\":\"Time\", \"y\":\"Conversion\"}) #title=\"Comparison of theoretical, semi-practical and practical <br>reaction fits for living polymerizations kinetics\",\n",
    "for curve_type, function in fitting_functions:\n",
    "    fitting_function_comparison_fig.add_scatter(x=x,y=function(x), mode=\"lines\", name=curve_type)\n",
    "\n",
    "fitting_function_comparison_fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        # showticklabels=False,# Hide tick labels on x-axis - sets the title setoff to 0, so the following workaround is done:\n",
    "        tickfont=dict(color=\"rgba(0,0,0,0)\", size=1),\n",
    "        showgrid=False,      # Hide grid lines\n",
    "        title=\"Time\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range=[0,1],\n",
    "        showticklabels=False,\n",
    "        showgrid=False,\n",
    "        title=\"Conversion\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ),\n",
    "    paper_bgcolor='white',  # Set background to light mode\n",
    "    plot_bgcolor='white',   # Set plot background to white\n",
    "    font=dict(color='black'),  # Set font color to black for light mode\n",
    "    margin=dict(l=0, r=0, t=0, b=0),  # Tight margins to reduce whitespace #t=80 with title\n",
    "    width=500, height=400,\n",
    ")\n",
    "# fitting_function_comparison_fig.write_image(os.path.join(os.getcwd(), \"data\", \"data exploration figures\", \"fitting_function_comparison_fig.svg\"))\n",
    "fitting_function_comparison_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reformatted_discarded[\"max_con\"] = 0\n",
    "combined_df_with_duplicates = pd.concat([reformatted_discarded, kinetics_df])\n",
    "combined_df_with_duplicates"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# search for duplicates of discarded and successful kinetics to look out for reproducibility and how different the experiments are\n",
    "groupby_params = kinetics_df.groupby([\"monomer\", \"RAFT-Agent\", \"solvent\"])\n",
    "groupby_params_labels = [f\"{solvent} with {monomer} in {_6}\" for solvent, monomer, _6 in groupby_params.groups.keys()]\n",
    "\n",
    "x = range(len(groupby_params_labels))\n",
    "y_max_conv = [group[\"max_con\"].values for _, group in groupby_params]\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, values in enumerate(y_max_conv):\n",
    "    ax.scatter([x[i]]*len(values), values,label=groupby_params_labels[i] if len(values) == 1 else \"\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(groupby_params_labels, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Max Conversion\")\n",
    "ax.set_xlabel(\"Solvent-Monomer-Raft\")\n",
    "ax.set_title(\"Max Conversion by Solvent, Monomer and RAFT-agent\")\n",
    "plt.tight_layout()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rafts = kinetics_df[\"RAFT-Agent\"].unique()\n",
    "for raft in rafts:\n",
    "    # raft_subset = kinetics_df[kinetics_df[\"RAFT-Agent\"] == raft]\n",
    "    raft_subset = combined_df_with_duplicates[combined_df_with_duplicates[\"RAFT-Agent\"] == raft]\n",
    "    groupby_params_rafts = raft_subset.groupby([\"monomer\", \"solvent\"])\n",
    "    groupby_params_labels_rafts = [f\"{monomer} in {solvent}\" for monomer, solvent in groupby_params_rafts.groups.keys()]\n",
    "    \n",
    "    x = range(len(groupby_params_labels_rafts))\n",
    "    y_max_conv = [group[\"max_con\"].values for _, group in groupby_params_rafts]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for i, values in enumerate(y_max_conv):\n",
    "        ax.scatter([x[i]]*len(values), values,label=groupby_params_labels_rafts[i] if len(values) == 1 else \"\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(groupby_params_labels_rafts, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Max Conversion\")\n",
    "    ax.set_xlabel(\"Solvent-Monomer\")\n",
    "    ax.set_title(f\"Max Conversion for all exp with {raft}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
