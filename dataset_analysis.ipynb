{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sort_RAFT_table as sRt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import re\n",
    "from scipy.optimize import curve_fit\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# rectify/balance the table\n",
    "sRt.df[\"t6h-conversion\"] = np.nan\n",
    "sRt.df[\"t10h-conversion\"] = np.nan\n",
    "\n",
    "# get all conversion headers, sort them by the hours\n",
    "conversion_list = []\n",
    "for column in sRt.df.columns:\n",
    "    if \"conversion\" in column:\n",
    "        conversion_list.append(column)\n",
    "\n",
    "conversion_list.sort(key=lambda x: int(re.findall(r\"\\d+\", x)[0]))\n",
    "\n",
    "# get all the Mn and Mw headers\n",
    "Mn_list = []\n",
    "Mw_list = []\n",
    "for column in sRt.df.columns:\n",
    "    if \"Mn\" in column:\n",
    "        Mn_list.append(column)\n",
    "    if \"Mw\" in column:\n",
    "        Mw_list.append(column)\n",
    "\n",
    "print(conversion_list, Mn_list, Mw_list, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def change_timeformat(time_format):\n",
    "    h_m_s = str(time_format).split(\":\")\n",
    "    m_format = int(h_m_s[0]) * 60 + int(h_m_s[1]) + int(h_m_s[2]) / 60\n",
    "    return m_format\n",
    "\n",
    "def change_timeformat_h(time_format):\n",
    "    h_m_s = str(time_format).split(\":\")\n",
    "    h_format = int(h_m_s[0]) + (int(h_m_s[1]) + int(h_m_s[2]) / 60) /60\n",
    "    return h_format\n",
    "\n",
    "# creating a table as a lookup to correct all sample measurement times\n",
    "\n",
    "# get the hours from the headers with regex\n",
    "hours_list = []\n",
    "two_digit_regex = r\"\\d+\"\n",
    "for column in conversion_list:\n",
    "    hours_list.append(int(re.findall(two_digit_regex, column)[0]))\n",
    "hours_list.sort()\n",
    "\n",
    "exact_times = pd.read_excel(sRt.INPUT_FILE_PATH, sheet_name=\"exact sampling times\")\n",
    "\n",
    "time_correction_df = pd.DataFrame(data=exact_times.iloc[3:12, 3:])\n",
    "time_correction_df.columns = exact_times.loc[2][3:]\n",
    "\n",
    "time_correction_df.reset_index(inplace=True, drop=True)\n",
    "ext_time_corr_df_data =[]\n",
    "for row in time_correction_df.iterrows():\n",
    "    reactor_nr = row[1][\"Reactor\"]\n",
    "    if type(reactor_nr) == int or reactor_nr == \"closing of reactors\": # that applies to reactor 15 and the closing of reactors\n",
    "        row[1][\"Reactor\"] = str(reactor_nr)\n",
    "        ext_time_corr_df_data.append(row[1])\n",
    "    else: # that applies to all reactors which are described per row in pairs like \"3+4\"\n",
    "        reactor_nr_s = (row[1][\"Reactor\"]).split(\"+\")\n",
    "        for reactor_nr in reactor_nr_s:\n",
    "            row[1][\"Reactor\"] = reactor_nr\n",
    "            ext_time_corr_df_data.append(row[1].copy())\n",
    "ext_time_corr_df = pd.DataFrame(data=ext_time_corr_df_data, columns=exact_times.loc[2][3:])\n",
    "ext_time_corr_df.reset_index(drop=True, inplace=True)\n",
    "print(\"The timer was reset to 0 after the first closing of reactors in the \\\"t = 0\\\"-sampling-step\")\n",
    "ext_time_corr_df.columns = [\"Reactor\", 0, 1, 2, 4, 6, 8, 10, 15]\n",
    "\n",
    "# change time format to minutes and set\n",
    "time_cols = ext_time_corr_df.columns.difference([\"Reactor\"])\n",
    "ext_time_corr_df[time_cols] = ext_time_corr_df[time_cols].apply(lambda x: [change_timeformat_h(d) for d in x])\n",
    "ext_time_corr_df[0] = ext_time_corr_df[0].apply(lambda x: 0 )\n",
    "\n",
    "# set index to reactor\n",
    "ext_time_corr_df.set_index(\"Reactor\", inplace=True)\n",
    "\n",
    "ext_time_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# replace the \"sample determiner\" with columns describing for the experiment number (next cell) and the actual reagents that made out the \"determiner\"\n",
    "reaction_descriptors_dict = {}\n",
    "abbreviation_keys = pd.read_excel(sRt.INPUT_FILE_PATH, sheet_name=\"Legend for Abbreviations\")\n",
    "abbreviation_keys.dropna(inplace=True)\n",
    "for row in abbreviation_keys.itertuples():\n",
    "    reaction_descriptors_dict[str(row.Symbol)] = row.Name\n",
    "\n",
    "reaction_descriptors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate a reformatted table with all entries attributing to a sample analysis taken, described with a column of the right time\n",
    "kinetic_curves = []\n",
    "for index, polymerisation_kinetic in sRt.df.iterrows():\n",
    "\n",
    "    kinetic_curve_entries = pd.DataFrame(index=range(len(polymerisation_kinetic[conversion_list])),\n",
    "        data={\"time\" : hours_list, \"conversion\" : polymerisation_kinetic[conversion_list].values,\n",
    "         \"Mn\" : polymerisation_kinetic[Mn_list].values, \"Mw\" : polymerisation_kinetic[Mw_list].values,\n",
    "         # , \"reactor\" : polymerisation_kinetic[\"reactor\"] reactor is not needed since the time is corrected\n",
    "         })\n",
    "\n",
    "    kinetic_curve_entries[\"exp_nr\"] = str(polymerisation_kinetic[\"Experiment number\"])\n",
    "    kinetic_curve_entries[\"monomer\"] = reaction_descriptors_dict[polymerisation_kinetic[\"monomer\"]]\n",
    "    kinetic_curve_entries[\"RAFT-Agent\"] = reaction_descriptors_dict[polymerisation_kinetic[\"RAFT-Agent\"]]\n",
    "    kinetic_curve_entries[\"solvent\"] = reaction_descriptors_dict[polymerisation_kinetic[\"solvent\"]]\n",
    "\n",
    "    # the times are dependent on the current reactor, get current\n",
    "    current_reactor_nr = str(polymerisation_kinetic[\"reactor\"])\n",
    "    current_time_list = ext_time_corr_df.loc[current_reactor_nr]\n",
    "    kinetic_curve_entries[\"time\"] = list(current_time_list)\n",
    "\n",
    "    kinetic_curves.append(kinetic_curve_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "''' ToDo: no good scientific practice and also the experiment numbers where updated - this one should now have the number 505\n",
    " # manually exclude experiment 301 as it disturbs the covariance plotting and has an unreasonable conversion value course.\n",
    "if kinetic_curves[301][\"exp_nr\"].iloc[1] == \"301\":\n",
    "    print(kinetic_curves[301])\n",
    "    fig = px.line(kinetic_curves[301].dropna(subset=\"conversion\"), x=\"time\", y=\"conversion\", title=f\"kinetic 301 \", labels={\"x\":\"Time\", \"y\":\"Conversion\"})\n",
    "    fig.add_scatter(x=kinetic_curves[301][\"time\"], y=kinetic_curves[301][\"conversion\"], mode=\"lines+markers\", line=dict(dash=\"dot\"), name=\"Mn\")\n",
    "    fig.show()\n",
    "    kinetic_curves.pop(301)\n",
    "else:\n",
    "    print(\"Cell was already run. Experiment is deleted.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "deprecated: more theoretical accurate version but practically worse applicable\n",
    "    # L is responsible for scaling the output range from [0,1] to [0,L]\n",
    "    # b\n",
    "    # k is responsible for scaling the input, which remains in (-inf,inf)\n",
    "    # x0\n",
    "def sigmoid (x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0)))+b\n",
    "    return (y)\n",
    "\n",
    "def sigmoid_derivative(x, L ,x0, k, b):\n",
    "    y = (L*k*np.exp(-k*(x-x0)))/((np.exp(-k*(x-x0))+1)**2)\n",
    "    return (y)\n",
    "\n",
    "\n",
    "'''\n",
    "def neg_growth(x, l, k):\n",
    "    y = l * (1 - np.exp(k * (-x)))\n",
    "    return y\n",
    "\n",
    "def neg_growth_derivative(x, l, k):\n",
    "    y = l * k * np.exp(k*(-x))\n",
    "    return y\n",
    "\n",
    "# as the Mn values do not all start at 0\n",
    "def neg_growth_abscissae(x, l, k, b):\n",
    "    y = l * (1 - np.exp(k * (-x))) + b\n",
    "    return y\n",
    "\n",
    "def linear_growth(x, m):\n",
    "    y = m * x\n",
    "    return y\n",
    "\n",
    "def linear_growth_derivative(m):\n",
    "    return m\n",
    "\n",
    "'''\n",
    "xrange = np.arange(-0.2, 5, 0.1)\n",
    "example_fig = px.line()\n",
    "for testparams in ([1,1], [2,1], [1,2]):\n",
    "    example_fig.add_scatter(x=xrange, y=neg_growth(xrange, *testparams))\n",
    "    example_fig.add_scatter(x=xrange, y=neg_growth_derivative(xrange, *testparams), opacity=0.5, line=dict(dash=\"dot\"))\n",
    "\n",
    "print(\"visualisation of the functions\")\n",
    "example_fig.show()\n",
    "# '''\n",
    "print(\"functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# checking out max and min of Mn/Mw\n",
    "min_Mn, max_Mn = [], []\n",
    "min_Mw, max_Mw = [], []\n",
    "\n",
    "for kc in kinetic_curves:\n",
    "    min_Mn.append(kc[\"Mn\"].min()); max_Mn.append(kc[\"Mn\"].max()); min_Mw.append(kc[\"Mw\"].min()); max_Mw.append(kc[\"Mw\"].max())\n",
    "\n",
    "for _ in (min_Mn, max_Mn, min_Mw, max_Mw):\n",
    "    _.sort()\n",
    "    _ = [j/100000 for j in _]\n",
    "\n",
    "molar_wg_comparison = px.line()\n",
    "molar_wg_comparison.add_scatter(y=min_Mw, name=\"min Mw\", line_color=\"darkgoldenrod\", opacity=0.8, fill=\"tozeroy\", fillcolor=\"orange\")\n",
    "molar_wg_comparison.add_scatter(y=max_Mw, name=\"max Mw\", line_color=\"darkgoldenrod\", opacity=0.8)\n",
    "molar_wg_comparison.add_scatter(y=min_Mn, name=\"min Mn\", line_color=\"firebrick\", opacity=0.8, fill=\"tozeroy\", fillcolor=\"red\")\n",
    "molar_wg_comparison.add_scatter(y=max_Mn, name=\"max Mn\", line_color=\"firebrick\", opacity=0.8)\n",
    "molar_wg_comparison.update_layout(title=\"Mn and Mw comparison\", yaxis_title=r\"$\\\\text{Mn and Mw }[g/mol] \\cdot 10^{-5}$\")\n",
    "molar_wg_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.line(title=\"Kinetic Curve Fit\")\n",
    "colors = px.colors.qualitative.Plotly # set up a simple color palette\n",
    "extended_xdata = np.linspace(-1, 16.5, 100) # x data array for plotting the fits\n",
    "\n",
    "\n",
    "def add_fits_to_plot(figure, fit_func, fit_func_params,  fit_func_derivative=None, *args, **kwargs):\n",
    "    figure.add_scatter(\n",
    "        x=extended_xdata, y=fit_func(extended_xdata, *fit_func_params),\n",
    "        opacity=1, line=dict(dash=\"dot\"), name=f\"{fit_func.__name__} fit\", *args, **kwargs)\n",
    "    if fit_func_derivative:\n",
    "        figure.add_scatter(\n",
    "            x=extended_xdata, y=fit_func_derivative(extended_xdata, *fit_func_params),\n",
    "            opacity=0.3, line=dict(dash=\"dash\"), name=f\"{fit_func_derivative.__name__}\", *args, **kwargs)\n",
    "\n",
    "def fit_and_exclude_outliers(x, y, fit_func, p0, bounds, nan_policy=\"omit\", iteration=1, outliers=None):\n",
    "    outliers = outliers if outliers is not None else []\n",
    "\n",
    "    # exclude the nan values from the data\n",
    "    mask = ~np.isnan(y)\n",
    "    x, y = x[mask], y[mask]\n",
    "\n",
    "    cf_data = curve_fit(f=fit_func, xdata=x, ydata=y, p0=p0, nan_policy=nan_policy, maxfev=800 * 10, bounds=bounds)\n",
    "\n",
    "    # calculate the fit points\n",
    "    fit_points = np.array([fit_func(x, *cf_data[0]) for x in x])\n",
    "    # calculate the standard deviation of the residuals between the fit and the data points\n",
    "    sigma = np.std(fit_points - y)\n",
    "\n",
    "    # exclude the outliers\n",
    "    msk = ~(np.abs(fit_points - y) > 2 * sigma)\n",
    "    if not msk.all():\n",
    "        return fit_and_exclude_outliers(x=x[msk], y=y[msk], fit_func=fit_func, p0=cf_data[0], bounds=bounds, nan_policy=nan_policy, iteration=iteration+1, outliers=outliers + [x[~msk]])\n",
    "\n",
    "    # calculate the squared error of fit and data points\n",
    "    sq_err = np.mean(np.square(y - fit_points))\n",
    "\n",
    "    result = {\"x\": x, \"y\": y, \"p_opt\" : cf_data[0], \"p_cov\" : cf_data[1], \"sq_err\": sq_err, \"excluded_points\" : (x[~msk],y[~msk]), \"iteration\" : iteration, \"outliers\" : outliers}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kinetics_df = pd.DataFrame(columns=['exp_nr', 'max_con', 'theo_max_con', 'theo_react_end', 'monomer',\n",
    "       'RAFT-Agent', 'solvent', 'fit_p1', 'fit_p2', 'p1_variance',\n",
    "       'p1_p2_covariance', 'p2_variance', 'squared_error', 'conv_time_data',\n",
    "       'Mn_time_data', 'Mw_time_data']) # create new dataframe with kinetics per row\n",
    "\n",
    "for idx, kinetic_curve in enumerate(kinetic_curves):\n",
    "    # first make sure the datapoints are in the right format and not sometimes int sometimes float\n",
    "    xdata = np.array(kinetic_curve[\"time\"].values, dtype=float)\n",
    "    ydata_conv = np.array(kinetic_curve[\"conversion\"].values, dtype=float)\n",
    "    ydata_Mn = np.array(kinetic_curve[\"Mn\"].values, dtype=float)/100000 # make it more comparable to conversion\n",
    "    ydata_Mw = np.array(kinetic_curve[\"Mw\"].values, dtype=float)/100000\n",
    "\n",
    "\n",
    "    # fitting section for conversion\n",
    "    p_initial = [max(ydata_conv), 0.1] # this is a mandatory initial guess\n",
    "    ng_fit = fit_and_exclude_outliers(x=xdata, y=ydata_conv, fit_func=neg_growth, p0=p_initial, bounds=([0, -np.inf], [1, np.inf]))\n",
    "    # l_fit = fit_and_exclude_outliers(x=xdata, y=ydata, fit_func=linear_growth, p0=[max(ydata)/7], bounds=([0], [np.inf]))\n",
    "\n",
    "    popt, pcov = ng_fit[\"p_opt\"], ng_fit[\"p_cov\"]\n",
    "    conv_time_data = np.array([ng_fit[\"x\"], ng_fit[\"y\"]])\n",
    "    squared_error = ng_fit[\"sq_err\"]\n",
    "\n",
    "    # fitting section for Mn\n",
    "    p_initial = [max(ydata_Mn[~np.isnan(ydata_Mn)]), 0.1, 0]\n",
    "    ng_fit_Mn = fit_and_exclude_outliers(x=xdata, y=ydata_Mn, fit_func=neg_growth_abscissae, p0=p_initial, bounds=([0, -np.inf, -np.inf], [1, np.inf, np.inf]))\n",
    "\n",
    "    popt_Mn, pcov_Mn = ng_fit_Mn[\"p_opt\"], ng_fit_Mn[\"p_cov\"]\n",
    "    Mn_time_data = np.array([ng_fit_Mn[\"x\"], ng_fit_Mn[\"y\"]])\n",
    "    squared_error_Mn = ng_fit_Mn[\"sq_err\"]\n",
    "\n",
    "    # fitting section for Mw\n",
    "    ng_fit_Mw = fit_and_exclude_outliers(x=xdata, y=ydata_Mw, fit_func=neg_growth_abscissae, p0=p_initial, bounds=([0, -np.inf, -np.inf], [1, np.inf, np.inf]))\n",
    "\n",
    "    popt_Mw, pcov_Mw = ng_fit_Mw[\"p_opt\"], ng_fit_Mw[\"p_cov\"]\n",
    "    Mw_time_data = np.array([ng_fit_Mw[\"x\"],ng_fit_Mw[\"y\"]])\n",
    "    squared_error_Mw = ng_fit_Mw[\"sq_err\"]\n",
    "\n",
    "\n",
    "    kinetics_df.loc[idx] = {\"exp_nr\":kinetic_curve[\"exp_nr\"].iloc[1], \"max_con\":max(ydata_conv),\n",
    "                        \"theo_max_con\":\"yet to calc\", \"theo_react_end\":\"yet to calc\",\n",
    "                            \"monomer\":kinetic_curve[\"monomer\"].iloc[1], \"RAFT-Agent\":kinetic_curve[\"RAFT-Agent\"].iloc[1],\n",
    "                            \"solvent\":kinetic_curve[\"solvent\"].iloc[1],\n",
    "                            \"fit_p1\":popt[0],\"fit_p2\":popt[1],\n",
    "                            \"p1_variance\":pcov[0][0],\"p1_p2_covariance\":pcov[0][1],\"p2_variance\":pcov[1][1],\n",
    "                            \"squared_error\":squared_error, \"conv_time_data\":conv_time_data,\n",
    "                            \"Mn_time_data\":Mn_time_data, \"Mw_time_data\":Mw_time_data}\n",
    "\n",
    "''' comment out here if plot is needed.\n",
    "    marker_dict = dict(color=colors[idx%len(colors)]) # set same colors per kinetic\n",
    "    fig.add_scatter(x=xdata, y=ydata, mode=\"lines+markers\", opacity=1, name=kinetic_curve[\"exp_nr\"].iloc[1], marker=marker_dict)\n",
    "    add_fits_to_plot(neg_growth, neg_growth_derivative, popt, marker_dict)\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        range=[-0.1,1]\n",
    "    ),\n",
    "\n",
    ")\n",
    "fig.show()\n",
    "# '''\n",
    "\n",
    "kinetics_df.reset_index(drop=True, inplace=True)\n",
    "kinetics_df.drop(axis=\"index\", index=kinetics_df[kinetics_df[\"max_con\"] <= 0].index, inplace=True)\n",
    "kinetics_df.reset_index(drop=True, inplace=True)\n",
    "# kinetics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot Mn Conversion difference\n",
    "fig = px.line(title=\"Mn Conversion difference\")\n",
    "data_type_list = [\"conv_time_data\", \"Mn_time_data\", \"Mw_time_data\"]\n",
    "data_list_name = [\"conv over time\", \"Mn over time\", \"Mw over time\"]\n",
    "for idx, data in (enumerate(data_type_list)):\n",
    "    for kin in kinetics_df[data].head(30):\n",
    "        fig.add_scatter(x=kin[0], y=kin[1], mode=\"lines+markers\", opacity=0.5, marker=dict(color=colors[idx%len(colors)]), showlegend=False, legendgroup=str(idx))\n",
    "    # custom legend\n",
    "    fig.add_scatter(x=[None], y=[None], marker=dict(color=colors[idx%len(colors)]), name= data_list_name[idx], legendgroup=str(idx))\n",
    "# adding a threshold line because the SEC peaks smear into syspeak at that point\n",
    "fig.add_scatter(x=[0, 16], y=[1000*10**(-5), 1000*10**(-5)], mode=\"lines\", line=dict(dash=\"dash\"), name=\"SEC limit\", legendgroup=\"threshold\")\n",
    "fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.05, xanchor=\"left\", x=-0.1 ), yaxis_title=r\"$\\\\text{Mn, Mw }[g/mol] \\cdot 10^{-5}/ \\ [ \\%]$\", xaxis_title=\"Time [h]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# normalize the errors by dividing them by their respective standard deviation\n",
    "def normalize_errors(err):\n",
    "    return err / np.std(err)\n",
    "\n",
    "for error in [\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"]:\n",
    "    kinetics_df[error] = normalize_errors(kinetics_df[error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# kinetics_df[kinetics_df[\"squared_error\"]>0.01] 29/317 entries\n",
    "# calculating covariance between errors to use only the reasonable ones\n",
    "\n",
    "# permutate all combinations of the errors\n",
    "errorcombs = [[],[],[]]\n",
    "for err1, err2 in itertools.combinations([\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"], 2):\n",
    "    # print(f\"The covariance between {err1} and {err2} is {np.cov(kinetics_df[err1], kinetics_df[err2])}\")\n",
    "    # print(f\"The correlation between {err1} and {err2} is {np.correlate(kinetics_df[err1], kinetics_df[err2])}\")\n",
    "    # print(\"\\n\")\n",
    "    errorcombs[0].append(f\"{err1}/{err2}\")\n",
    "    errorcombs[1].append(np.cov(kinetics_df[err1], kinetics_df[err2]))\n",
    "    errorcombs[2].append(*np.correlate(kinetics_df[err1], kinetics_df[err2]))\n",
    "errorcombs_dc = {\"name\": errorcombs[0], \"covariance\":errorcombs[1], \"correlation\":errorcombs[2]}\n",
    "errorcombs_df = pd.DataFrame(data=errorcombs_dc)\n",
    "errorcombs_df[\"i_correlation\"] = errorcombs_df[\"correlation\"] * (-1)\n",
    "\n",
    "# plot bar plot per a pair of errors with superimposed correlation\n",
    "err_fig = px.bar(title=\"Correlation between Errors\", labels={\"correlation\":\"correlation\"}, log_y=True)\n",
    "err_fig.add_bar(x=errorcombs_df[\"name\"], y=errorcombs_df[\"correlation\"], name=\"positive correlation\", marker_color=\"green\")\n",
    "err_fig.add_bar(x=errorcombs_df[\"name\"], y=errorcombs_df[\"i_correlation\"], name=\"negative correlation\", marker_color=\"crimson\")\n",
    "err_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# violin plots of the errors\n",
    "for errors in [\"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"]:\n",
    "    kinetics_df[errors] = kinetics_df[errors].apply(lambda x: np.absolute(x))\n",
    "fig = px.violin(kinetics_df, y=[\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"], box=True, points=\"all\", log_y=True)\n",
    "fig.update_layout(title=\"Errors normalize by \\u03C3\", xaxis_title=\"Error type\", yaxis_title=\"value\", yaxis=dict(title=\"log(value)\", range=(-22, 3)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get information about within which error margin the fits and therefore kinetics are good\n",
    "# split error in quartiles, index per error\n",
    "\n",
    "title_dic = {\"squared_error\":\"r² - error\",\n",
    "             \"p1_variance\":\"variance of parameter 1\",\n",
    "             \"p2_variance\":\"variance of parameter 2\",\n",
    "             \"p1_p2_covariance\":\"covariance (Params 1 & 2)\"}\n",
    "\n",
    "def get_quartile_indexes(error_type):\n",
    "    quartile_len = len(kinetics_df) / 4\n",
    "    quartile_ranges = np.array([(a * quartile_len, (a + 1) * quartile_len) for a in range(4)], dtype=int)\n",
    "    quartiles_list = list()\n",
    "    for q in range(4):\n",
    "        quartiles_list.append(kinetics_df.sort_values(by=[error_type]).iloc[quartile_ranges[q][0]:quartile_ranges[q][1]])\n",
    "    return quartiles_list\n",
    "\n",
    "def export_quartile_figures():\n",
    "    for err in tqdm([\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"]):\n",
    "        err_quartiles = get_quartile_indexes(err)\n",
    "\n",
    "        # plot the single quartiles\n",
    "        for nr, n in enumerate(err_quartiles):\n",
    "            iks = n.index\n",
    "            fg = px.line(title=f\"kinetics (quartile {nr+1} of the {title_dic[err]}) \", labels={\"x\":\"time\", \"y\":\"conversion\"})\n",
    "            for ik in iks:\n",
    "                marker_dict = dict(color=colors[ik%len(colors)])\n",
    "                fit_data = kinetics_df.iloc[ik]\n",
    "                xdt, ydt = fit_data[\"xdata\"], fit_data[\"ydata\"]\n",
    "                fg.add_scatter(x=xdt, y=ydt, mode=\"lines+markers\", name=kinetic_curves[ik][\"exp_nr\"].iloc[0], marker=marker_dict)\n",
    "                add_fits_to_plot(fig, neg_growth, neg_growth_derivative, [fit_data[\"fit_p1\"], fit_data[\"fit_p2\"]], marker=marker_dict)\n",
    "                fg.update_layout(yaxis=dict(range=[-0.1,1]), xaxis_title=\"Time [h]\", yaxis_title=\"Conversion [%]\")\n",
    "            fg.write_image(f\"data exploration figures/{err} ({nr+1} quartile).svg\")\n",
    "\n",
    "# export_quartile_figures() ; print(\"exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# descry when a function aligns to the datapoints in a reasonable way\n",
    "#   Hence, wheneth' the blunder exaggerates, an 80% betweeneth' of the maximum conversion in that kinetic should be assessed to be the maximum conversion.\n",
    "#   let's give a point for every quartile further from the first for the single errors divided by the maximum score (that is 3*4=12)\n",
    "error_list = [\"squared_error\", \"p1_variance\", \"p2_variance\", \"p1_p2_covariance\"]\n",
    "error_dic = {}\n",
    "score = np.zeros(len(kinetics_df), int)\n",
    "for error in error_list:\n",
    "    quartiles = get_quartile_indexes(error)\n",
    "    # for every error we want to give an error per index\n",
    "    for sc, quartile in enumerate(quartiles):\n",
    "        if sc == 0:\n",
    "            continue\n",
    "        # each quartile is a dataframe, where the latter one raise a higher error ( 0, 1, 2, 3)\n",
    "        for num in quartile.index:\n",
    "            score[num]+=sc\n",
    "kinetics_df[\"error_score\"]=score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p1_values = kinetics_df[\"fit_p1\"].values\n",
    "average_conversion = p1_values.mean()\n",
    "min_con, max_con = p1_values.min(), p1_values.max()\n",
    "\n",
    "print(f\"According to the fits the mean maximum conversion is {average_conversion:.3n}, the minimum maximum is {min_con:.3n} and the maximum overall is {max_con:.3n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# determining apposite reaction end values.\n",
    "# The moment where 90% of the maximum conversion have been reached can be seen as a practical maximum conversion point\n",
    "kinetics_df[\"theo_max_con\"] = kinetics_df[\"fit_p1\"].apply(lambda x: x*0.9)\n",
    "\n",
    "def y_converted_negative_growth(y, l, k):\n",
    "    return -np.log(1 - y / l) / k\n",
    "\n",
    "kinetics_df[\"theo_react_end\"] = \\\n",
    "    [y_converted_negative_growth(y, fit_p1, fit_p2) for y, fit_p1, fit_p2 in zip(kinetics_df[\"theo_max_con\"], kinetics_df[\"fit_p1\"], kinetics_df[\"fit_p2\"])]\n",
    "\n",
    "# the theoretical maximal conversion must be capped at reasonable time (we take two days here) that is applying 139/313 entries\n",
    "kinetics_df[\"theo_react_end\"] = [30 if x > 30 else x for x in kinetics_df[\"theo_react_end\"].values]\n",
    "# recalculate the apposite maximal conversion\n",
    "kinetics_df[\"theo_max_con\"] = [neg_growth(x, p1, p2) for x, p1, p2 in zip(kinetics_df[\"theo_react_end\"], kinetics_df[\"fit_p1\"], kinetics_df[\"fit_p2\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# searching manually for interesting experiments to be reproduced by MR.\n",
    "search_queue_monomer = kinetics_df[\"monomer\"].apply(lambda x: x in [\"Styrene\",  \"4-Methylstyrene\", \"Benzyl acrylate\"])\n",
    "search_queue_solvent = kinetics_df[\"solvent\"].apply(lambda x: x in [\"Dimethylformamide\"])\n",
    "kinetics_df[search_queue_monomer & search_queue_solvent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# one longrun test for the theo react end cap (146) for up to 72h\n",
    "# two to see reproducible exp from within the second quantile in average (score of 4/5)\n",
    "#   one \"fast\" fit point (202)\n",
    "#   one for a weak conversion (90)\n",
    "kinetics_df[kinetics_df['exp_nr'].isin([\"205\", \"90\", \"146\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to find the optimal threshold parameters for search one has to keep in mind that with high conversion (assuming around 80%, research/citation needed) increasing side reaction take place. After that the reactions should be sorted after time then error score. Maybe a multiple-decreasing-threshold-sorting-algorithm would be good.\n",
    "# so first priority would be sorting after nearest to 80% conversion.\n",
    "\n",
    "# create a score ingesting the importance of the different kinetic descriptors\n",
    "    # Conversion*1 + time*(-0.8) + error_score*(0.5)\n",
    "    # while spanning between the optimum and the least bearable values like in the following:\n",
    "        #  Conversion: |con-0.8| - 0 (0.8 is the optimum)\n",
    "        #     using a linear decreasing function -x*m+b\n",
    "        #  Time: 0 - np.inf (0 is the optimum) (more than 72 is not bearable)\n",
    "        #     using a negative potential function -x**2+b\n",
    "        #  Error: 0 - 12 (0 is the optimum) (the error is more negligible)\n",
    "        #     using a linear decreasing function -x*m+b\n",
    "score = []\n",
    "for row in kinetics_df.itertuples():\n",
    "    score.append(((0.8-np.abs(row.theo_max_con - 0.8))/0.8*1 + (-(row.theo_react_end/72)**2+1)*0.8 + ((12-row.error_score)/12)*0.5))\n",
    "kinetics_df[\"score\"] = score\n",
    "kinetics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interface:\n",
    "- Give in Monomer ~~and Mn~~\n",
    "    - The dataset will then be filtered for that one and the results, sorted via conversion (100% conversion, 80% time, 50% Fit) be given out.\n",
    "        - Tabular data with kinetic curve and fit as hover, reaction properties (solvent, Raft agent, time...)\n",
    "- Table should be interactive,\n",
    "- optionally, solvent and or raft agent can be chosen\n",
    "- Mn search is also just optionally after \"main\" search, as the Mn data is hard to plot...\n",
    "- if the queued monomer is not in the dataset the closest ones should be searched for and displayed instead.\n",
    "- same goes with solvent via polarity comparison maybe just as a highly experimental suggestion: \"Suggestion: Less polar solvents than Toluene might prove more useful but are not tried out in the dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def color_variant(hex_color, brightness_offset=1):\n",
    "    \"\"\" takes a color like #87c95f and produces a lighter or darker variant \"\"\"\n",
    "    if len(hex_color) != 7:\n",
    "        raise Exception(\"Passed %s into color_variant(), needs to be in #87c95f format.\" % hex_color)\n",
    "    rgb_hex = [hex_color[x:x+2] for x in [1, 3, 5]]\n",
    "    new_rgb_int = [int(hex_value, 16) + brightness_offset for hex_value in rgb_hex]\n",
    "    new_rgb_int = [min([255, max([0, i])]) for i in new_rgb_int] # make sure new values are between 0 and 255\n",
    "    # hex() produces \"0x88\", we want just \"88\"; also we zfill to pad with leading zeros if necessary, e.g. 9 -> 09\n",
    "    return \"#\" + \"\".join([hex(i)[2:].zfill(2) for i in new_rgb_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some search functions:\n",
    "def search_for_exp(exp_nr: str | list) -> pd.DataFrame:\n",
    "    if type(exp_nr) == str:\n",
    "        print(f\"Searching for experiment {exp_nr}\")\n",
    "\n",
    "        return kinetics_df[kinetics_df[\"exp_nr\"] == exp_nr]\n",
    "    else:\n",
    "        return kinetics_df[kinetics_df[\"exp_nr\"].isin(exp_nr)]\n",
    "\n",
    "def plot_exp(exp_nr: str | list, plot_mn: bool=False, plot_mw: bool=False, fit_curves: tuple=(True, True)):\n",
    "    plot_data = search_for_exp(exp_nr)\n",
    "    exp_fig = px.line(title=f\"Kinetic Curve Fit for {exp_nr}\")\n",
    "    for kinetic_to_plot in plot_data.itertuples():\n",
    "        x_data, ydata = kinetic_to_plot.conv_time_data\n",
    "        marker_dict = dict(color=colors[int(kinetic_to_plot.Index)%len(colors)])\n",
    "        exp_fig.add_scatter(x=x_data, y=ydata, mode=\"lines+markers\", name=kinetic_to_plot.exp_nr, marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr))\n",
    "        if fit_curves[0]:\n",
    "            if fit_curves[1]:\n",
    "                add_fits_to_plot(exp_fig, neg_growth, [kinetic_to_plot.fit_p1, kinetic_to_plot.fit_p2], fit_func_derivative=neg_growth_derivative, marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr), showlegend=False)\n",
    "            else:\n",
    "                add_fits_to_plot(exp_fig, neg_growth, [kinetic_to_plot.fit_p1, kinetic_to_plot.fit_p2], marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr), showlegend=False)\n",
    "\n",
    "        if plot_mn:\n",
    "            marker_dict[\"color\"] = color_variant(marker_dict[\"color\"], 30)\n",
    "            x2_data, y2_data = kinetic_to_plot.Mn_time_data\n",
    "            exp_fig.add_scatter(x=x2_data, y=y2_data, mode=\"lines+markers\", name=\"Mn of \" + kinetic_to_plot.exp_nr, marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr))\n",
    "        if plot_mw:\n",
    "            marker_dict[\"color\"] = color_variant(marker_dict[\"color\"], -60)\n",
    "            x2_data, y2_data = kinetic_to_plot.Mw_time_data\n",
    "            exp_fig.add_scatter(x=x2_data, y=y2_data, mode=\"lines+markers\", name=\"Mw of \" + kinetic_to_plot.exp_nr, marker=marker_dict, legendgroup=str(kinetic_to_plot.exp_nr), opacity=0.5)\n",
    "\n",
    "    exp_fig.update_layout(yaxis=dict(range=[-0.1,1]), xaxis_title=\"Time [h]\", yaxis_title=\"Conversion [%]\")\n",
    "    exp_fig.show()\n",
    "\n",
    "def find_optimal_synthesis(monomer: str | list):\n",
    "    search_q_monomer = [x in monomer for x in kinetics_df[\"monomer\"]]\n",
    "    result_df = kinetics_df[search_q_monomer].sort_values(by=[\"score\"], ascending=False)\n",
    "    return result_df\n",
    "\n",
    "def refine_search(dataframe: pd.DataFrame, monomer: list = None, solvent: list = None, raft_agent: list = None):\n",
    "    len_df = len(dataframe)\n",
    "    search_q_monomer, search_q_solvent, search_q_raft_agent = [np.array([True] * len_df) for _ in range(3)]\n",
    "    if monomer:\n",
    "        search_q_monomer = dataframe[\"monomer\"].apply(lambda x: x in [*monomer])\n",
    "    if solvent:\n",
    "        search_q_solvent = dataframe[\"solvent\"].apply(lambda x: x in [*solvent])\n",
    "    if raft_agent:\n",
    "        search_q_raft_agent = dataframe[\"RAFT-Agent\"].apply(lambda x: x in [*raft_agent])\n",
    "    return dataframe[search_q_monomer & search_q_solvent & search_q_raft_agent]\n",
    "\n",
    "# search_for_exp([\"145\", \"90\", \"253\"])\n",
    "# plot_exp([\"145\", \"90\", \"253\"], True, True)\n",
    "plot_exp([\"241\", \"146\", \"392\"], True, True)\n",
    "# find_optimal_synthesis([\"Styrene\", \"4-Methylstyrene\", \"Benzyl acrylate\"])\n",
    "# refine_search(kinetics_df, solvent=[\"Dimethylformamide\"], monomer=[\"Styrene\", \"4-Methylstyrene\", \"Benzyl acrylate\"], raft_agent=[\"2-Cyan-2-propylbenzodithioat\"])\n",
    "# refine_search(find_optimal_synthesis([\"Styrene\", \"4-Methylstyrene\", \"Benzyl acrylate\"]), solvent=[\"Dimethylformamide\"], raft_agent=[\"2-Cyan-2-propylbenzodithioat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import panel as pn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pn.extension()\n",
    "wind_speed = pn.widgets.FloatSlider(\n",
    "    value=5, start=0, end=20, step=1, name=\"Wind Speed (m/s)\"\n",
    ")\n",
    "\n",
    "efficiency = 0.3\n",
    "\n",
    "power = wind_speed.rx() * efficiency\n",
    "\n",
    "power_text = pn.rx(\n",
    "    \"Wind Speed: {wind_speed} m/s, \"\n",
    "    \"Efficiency: {efficiency}, \"\n",
    "    \"Power Generation: {power:.1f} kW\"\n",
    ").format(wind_speed=wind_speed, efficiency=efficiency, power=power)\n",
    "\n",
    "pn.Column(power_text).servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def display_color(hex_color, width=6):\n",
    "    if type(hex_color) == str:\n",
    "        display(Markdown(f'<span style=\"font-family: monospace\">{hex_color} <span style=\"color: {hex_color}\">{chr(9608)*width}</span></span>'))\n",
    "    else:\n",
    "        display(Markdown('<br>'.join(\n",
    "            f'<span style=\"font-family: monospace\">{color} <span style=\"color: {color}\">{chr(9608)*width}</span></span>'\n",
    "            for color in hex_color)))\n",
    "display_color(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# manually typing in the proof experimental data from michael\n",
    "# \"MRG-046-G-ZL-1-E-DMF\" -> exp\n",
    "# \"MRG-046-G-ZL-4-B-DMF\" -> exp\n",
    "# \"MRG-046-G-ZL-15-F-DMF\" -> exp\n",
    "# \"exp\", \"monomer\", \"RAFT-Agent\", \"solvent\", \"time\", \"conversion\", \"Mn\", \"Mw\"\n",
    "\n",
    "\n",
    "\"\"\" !The experiment numbers changed to from 146, 90 and 205 to 241, 145 and 343!\n",
    "M. Ringleb:\n",
    "146 evaluation of the observations was done after ca. 90 hno addition of NMR solvent and SEC eluent to vials for 15 h sample -->\n",
    "potential eNAplanation for problems with evaluation or deviations (sample stood for 5 h without being quenched) --> for NMR additon of 400 yL of CDCl3 prior to filling to NMR tube\n",
    "\n",
    "90 evaluation of this points was done after 20 h time in the reactor\n",
    "double addition of NMR solvent and SEC eluent to the vials for t =10h\n",
    "also for NMR sampling for t=10 h sample the septum was pushed through the lid, so the vial stood open for ca. 9hours before it was filled to the NMR tube \"\n",
    "\n",
    "205 evaluation of this points was done after 20 h time in the reactor\"\"\"\n",
    "\n",
    "proof_data = [\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 0, 0.0, 0.0, 0.0],\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 15, 0.05, 3000, 3400],\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 60, 0.24, 3500, 3900],\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 72, 0.24, 3500, 4100],\n",
    "    [\"241b\", \"1\", \"E\", \"DMF\", 84, 0.24, 3700, 4200],\n",
    "    [\"145b\", \"4\", \"B\", \"DMF\", 0, 0.0, 0.0, 0.0],\n",
    "    [\"145b\", \"4\", \"B\", \"DMF\", 2.95, 0.02, 870, 970],\n",
    "    [\"145b\", \"4\", \"B\", \"DMF\", 10, 0.10, 2200, 2500],\n",
    "    [\"343b\", \"15\", \"F\", \"DMF\", 0, 0.0, 0.0, 0.0],\n",
    "    [\"343b\", \"15\", \"F\", \"DMF\", 5.62, 0.85, 7700, 14800],\n",
    "    [\"343b\", \"15\", \"F\", \"DMF\", 8, 0.90, 7100, 15100],\n",
    "    ]\n",
    "\n",
    "proof_df_points = pd.DataFrame(data=proof_data, columns=[\"exp_nr\", \"monomer\", \"RAFT-Agent\", \"solvent\", \"time\", \"conversion\", \"Mn\", \"Mw\"])\n",
    "# reformatting to kinetics_df format\n",
    "proof_df = pd.DataFrame()\n",
    "for kinetic_nr in proof_df_points[\"exp_nr\"].unique():\n",
    "    kinetic = proof_df_points[proof_df_points[\"exp_nr\"] == kinetic_nr]\n",
    "    proof_kinetic = pd.DataFrame({\"exp_nr\":kinetic[\"exp_nr\"].iloc[0],\n",
    "                                  \"monomer\":reaction_descriptors_dict[kinetic[\"monomer\"].iloc[0]],\n",
    "                                  \"RAFT-Agent\":reaction_descriptors_dict[kinetic[\"RAFT-Agent\"].iloc[0]],\n",
    "                                  \"solvent\":reaction_descriptors_dict[kinetic[\"solvent\"].iloc[0]],\n",
    "\n",
    "                                  \"conv_time_data\":[[kinetic[\"time\"].values, kinetic[\"conversion\"].values]],\n",
    "                                  \"Mn_time_data\":[[kinetic[\"time\"].values, kinetic[\"Mn\"].values/100000]],\n",
    "                                  \"Mw_time_data\":[[kinetic[\"time\"].values, kinetic[\"Mw\"].values/100000]]})\n",
    "\n",
    "    proof_df = pd.concat([proof_df, proof_kinetic])\n",
    "proof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# comparing proof and original experiments data\n",
    "fig = px.line()\n",
    "\n",
    "for idx, data in (enumerate(data_type_list)):\n",
    "    comb_d = pd.concat([proof_df, search_for_exp([\"241\", \"145\", \"343\"])])\n",
    "    for kin, exp_nr in zip(comb_d[data], comb_d[\"exp_nr\"]):\n",
    "        fig.add_scatter(x=kin[0], y=kin[1], mode=\"lines+markers\", opacity=0.5, name=exp_nr, marker=dict(color=colors[idx%len(colors)]), showlegend=False, legendgroup=str(idx))\n",
    "    # custom legend\n",
    "    fig.add_scatter(x=[None], y=[None], marker=dict(color=colors[idx%len(colors)]), name=data_list_name[idx], legendgroup=str(idx))\n",
    "# adding a threshold line because the SEC peaks smear into syspeak at that point\n",
    "fig.add_scatter(x=[0, 84], y=[1000*10**(-5), 1000*10**(-5)], mode=\"lines\", line=dict(dash=\"dash\"), name=\"SEC limit\", legendgroup=\"threshold\")\n",
    "fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.05, xanchor=\"left\", x=-0.1 ), yaxis_title=r\"$Mn, Mw \\ [g/mol] \\cdot 10^{-5}\\ and \\ conversion \\ [ \\%]$\", xaxis_title=\"Time [h]\")\n",
    "fig.show()\n",
    "\"\"\"\n",
    "Y. Köster:\n",
    "The Comparison of proof and prior data shows that the experimental data is not easily reproducible with respective accuracy in small orders of magnitude (Conversion after 15 h around 10 %)\n",
    "Especially for the RAFT synthesis with this concern of low reaction rate an extrapolation further than double the time (to 30 h) seems not feasible.\n",
    " Therefore the former time cap of 70 h gets changed to 30 h.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# search for all kinetics with falling Mn, Mw Values, they start appearing a lot when the Mn is over 0.25 * 10**5\n",
    "search_q_falling_Mn = [True if np.mean(kin[1]) > 0.45 else False for kin in kinetics_df[\"Mn_time_data\"]]\n",
    "# plot_exp(kinetics_df[search_q_falling_Mn][\"exp_nr\"].values, True, True,  fit_curves=(False, False))\n",
    "print(\"The sinking Mn/Mw got all curated out\") # ToDo plot them via the discarded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# how many number unique did it actually make through curation\n",
    "interests = [\"monomer\", \"RAFT-Agent\", \"solvent\"]\n",
    "for interest in interests:\n",
    "    print(str(kinetics_df[interest].nunique()) + \" for \" + interest)\n",
    "    print(kinetics_df[interest].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "refine_search(dataframe=kinetics_df, raft_agent=[\"Benzyl 1H-pyrrole-1-carbodithioate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_exp([\"241\", \"145\", \"343\"], True, True)\n",
    "search_for_exp([\"241\", \"145\", \"343\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trying to parse PSMILES to explicit SMILES\n",
    "from rdkit import Chem\n",
    "PSMILES = \"[*]CC([*])(C)(C(=O)OC)\"\n",
    "explicit_smiles = \"[CH2][C](C)(C(=O)OC)\"\n",
    "# stripped_PSMILES = PSMILES.replace(\"[*]\", \"\")\n",
    "stripped_PSMILES = \"CC(C)(C(=O)OC)\"\n",
    "\n",
    "\n",
    "PSMILES_mol = Chem.MolFromSmiles(PSMILES)\n",
    "explicit_smiles_mol = Chem.MolFromSmiles(explicit_smiles)\n",
    "stripped_PSMILES_mol = Chem.MolFromSmiles(stripped_PSMILES)\n",
    "\n",
    "for atom in PSMILES_mol.GetAtoms():\n",
    "    print(str(atom.GetIdx()) +\" \"+ atom.GetSymbol() + \" has \" + str(atom.GetExplicitValence()) + \" explicit valence\")\n",
    "print(PSMILES, \" PSMILES\")\n",
    "display(PSMILES_mol)\n",
    "\n",
    "for atom in explicit_smiles_mol.GetAtoms():\n",
    "    print(str(atom.GetIdx()) +\" \"+ atom.GetSymbol() + \" has \" + str(atom.GetExplicitValence()) + \" explicit valence\")\n",
    "print(explicit_smiles, \" explicit_smiles\")\n",
    "display(explicit_smiles_mol)\n",
    "\n",
    "for atom in stripped_PSMILES_mol.GetAtoms():\n",
    "    print(str(atom.GetIdx()) +\" \"+ atom.GetSymbol() + \" has \" + str(atom.GetExplicitValence()) + \" explicit valence\")\n",
    "print(stripped_PSMILES, \" stripped_PSMILES\")\n",
    "display(stripped_PSMILES_mol)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(\"S\"+explicit_smiles*2+\"S\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_p_to_explicit_smiles(psmiles: str) -> str:\n",
    "    # first create a mol object\n",
    "    print(f\"Parsing {psmiles}\")\n",
    "    mol = Chem.MolFromSmiles(PSMILES)\n",
    "    # iterate over the atoms catching the position of the C Atoms following and preceding [*] and their valence\n",
    "    star_index = []\n",
    "    valence = []\n",
    "    atomsymbols = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atomsymbol = atom.GetSymbol()\n",
    "        atomsymbols.append(atomsymbol)\n",
    "        if atomsymbol == \"*\":\n",
    "            star_index.append(atom.GetIdx())\n",
    "        valence.append(atom.GetExplicitValence())\n",
    "    c_index = [star_index[0]+1, star_index[1]-1]\n",
    "\n",
    "    if len(star_index) > 2:\n",
    "        # raise a type error\n",
    "        raise TypeError (\"{__name__} cannot handle ladder polymers/more than two [*] in the PSMILES string!\")\n",
    "\n",
    "    # the current valence of the C atoms is the explicit minus the [*] connection/-1\n",
    "\n",
    "    h_num = valence\n",
    "    for idx in c_index:\n",
    "        h_num[idx] = 4 - valence[idx]\n",
    "    # replace C atoms with the explicit hydrogen count and catch the special case of parenthesised like e.g.([*]) and parenthesise the dangling part of the smiles string to the end after that C atom like [*]CC([*])(C)C(=O)OC -> [CH2][C](C)(C(=O)OC) (attention to the bracket pair ending with the last symbol \")\")\n",
    "    #     print(f\"C ({atomsymbols[idx]} at pos {idx}) should have valence {valence[idx]} H atoms appended\")\n",
    "\n",
    "    # create a dict of atom index and index of it's symbol in the string\n",
    "    import re\n",
    "    atom_positions = [m.start() for m in re.finditer(\"[A-Za-z*]\", psmiles)] # this does not work with [Si] for example\n",
    "\n",
    "    atom_string_map = {atom_symbol_nr:string_pos for atom_symbol_nr, string_pos in zip(range(len(atomsymbols)), atom_positions)}\n",
    "\n",
    "    c_str_indexes = [atom_string_map[_c_index] for _c_index in c_index]\n",
    "\n",
    "    c_str_h_num = {atom_string_map[_c_index]:h_num[_c_index] for _c_index in c_index}\n",
    "\n",
    "    smiles_reconstruction = \"\"\n",
    "    for idx, character in enumerate(psmiles):\n",
    "        if idx in c_str_indexes:\n",
    "            smiles_reconstruction += f\"[CH{c_str_h_num[idx]}]\" if c_str_h_num[idx] > 0 else \"[C]\"\n",
    "        else:\n",
    "            smiles_reconstruction += character\n",
    "\n",
    "\n",
    "    smiles_reconstruction = smiles_reconstruction.replace(\"([*])\", \"\")\n",
    "    smiles_reconstruction = smiles_reconstruction.replace(\"[*]\", \"\")\n",
    "    smiles_reconstruction = smiles_reconstruction.replace(\"*\", \"\")\n",
    "\n",
    "    return smiles_reconstruction\n",
    "parse_p_to_explicit_smiles(PSMILES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"[CH2][C](C)(C(=O)OC)\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(\"S\"+\"OC(CCC(c1ccccc1))CC(=O)O\"*1+\"S\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}